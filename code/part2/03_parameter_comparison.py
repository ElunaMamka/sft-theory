"""
æ¼”ç¤ºä»£ç ï¼šå‚æ•°é‡è¯¦ç»†å¯¹æ¯”
æ–‡ä»¶ä½ç½®ï¼šcode/part2/03_parameter_comparison.py

ç”¨å…·ä½“æ•°å­—å±•ç¤ºä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹ï¼ŒLoRA vs å…¨é‡å¾®è°ƒçš„å‚æ•°å·®å¼‚
"""

import matplotlib.pyplot as plt
import numpy as np


class ParameterComparison:
    """å‚æ•°é‡å¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self):
        # å¸¸è§æ¨¡å‹è§„æ¨¡
        self.model_sizes = {
            '7B': 7_000_000_000,
            '13B': 13_000_000_000,
            '30B': 30_000_000_000,
            '70B': 70_000_000_000
        }
        
        # LoRAé…ç½®
        self.lora_config = {
            'rank': 8,  # LoRAç§©
            'alpha': 16,
            'num_layers': 32,  # åº”ç”¨LoRAçš„å±‚æ•°
            'dim': 4096  # éšè—å±‚ç»´åº¦
        }
    
    def calculate_lora_params(self, model_size_name: str) -> dict:
        """
        è®¡ç®—LoRAå‚æ•°é‡
        
        Args:
            model_size_name: æ¨¡å‹è§„æ¨¡åç§°ï¼ˆå¦‚ '7B'ï¼‰
            
        Returns:
            å‚æ•°ç»Ÿè®¡å­—å…¸
        """
        total_params = self.model_sizes[model_size_name]
        
        # æ¯å±‚LoRAå‚æ•°ï¼š(d * r) + (r * d) = 2 * d * r
        params_per_layer = 2 * self.lora_config['dim'] * self.lora_config['rank']
        
        # æ€»LoRAå‚æ•°
        total_lora_params = params_per_layer * self.lora_config['num_layers']
        
        # è®¡ç®—æ¯”ä¾‹
        ratio = (total_lora_params / total_params) * 100
        
        return {
            'æ¨¡å‹æ€»å‚æ•°': total_params,
            'LoRAå‚æ•°': total_lora_params,
            'è®­ç»ƒå‚æ•°æ¯”ä¾‹': ratio,
            'æ¯å±‚å‚æ•°': params_per_layer,
            'åº”ç”¨å±‚æ•°': self.lora_config['num_layers']
        }
    
    def print_detailed_comparison(self):
        """æ‰“å°è¯¦ç»†å¯¹æ¯”"""
        print("="*80)
        print("ğŸ“Š å…¨é‡å¾®è°ƒ vs LoRA - å‚æ•°é‡è¯¦ç»†å¯¹æ¯”")
        print("="*80)
        
        print(f"\nğŸ”§ LoRAé…ç½®ï¼š")
        print(f"  â€¢ ç§© (rank): {self.lora_config['rank']}")
        print(f"  â€¢ Alpha: {self.lora_config['alpha']}")
        print(f"  â€¢ åº”ç”¨å±‚æ•°: {self.lora_config['num_layers']}")
        print(f"  â€¢ éšè—ç»´åº¦: {self.lora_config['dim']}")
        
        print(f"\n{'='*80}")
        print(f"{'æ¨¡å‹è§„æ¨¡':<12} {'å…¨é‡å¾®è°ƒå‚æ•°':<20} {'LoRAå‚æ•°':<20} {'è®­ç»ƒæ¯”ä¾‹':<15} {'å‡å°‘å€æ•°':<10}")
        print(f"{'='*80}")
        
        for model_name in self.model_sizes.keys():
            stats = self.calculate_lora_params(model_name)
            
            full_params = stats['æ¨¡å‹æ€»å‚æ•°']
            lora_params = stats['LoRAå‚æ•°']
            ratio = stats['è®­ç»ƒå‚æ•°æ¯”ä¾‹']
            reduction = full_params / lora_params
            
            print(f"{model_name:<12} {full_params:<20,} {lora_params:<20,} {ratio:<14.3f}% {reduction:<10.1f}x")
        
        print(f"{'='*80}")
    
    def calculate_storage_savings(self):
        """è®¡ç®—å­˜å‚¨èŠ‚çœ"""
        print(f"\n{'='*80}")
        print("ğŸ’¾ å­˜å‚¨ç©ºé—´å¯¹æ¯”ï¼ˆ10ä¸ªä¸åŒä»»åŠ¡ï¼‰")
        print(f"{'='*80}")
        
        num_tasks = 10
        
        print(f"\n{'æ¨¡å‹è§„æ¨¡':<12} {'å…¨é‡å¾®è°ƒå­˜å‚¨':<20} {'LoRAå­˜å‚¨':<20} {'èŠ‚çœç©ºé—´':<15}")
        print(f"{'-'*80}")
        
        for model_name in self.model_sizes.keys():
            stats = self.calculate_lora_params(model_name)
            
            # FP16: 2 bytes per parameter
            full_storage_gb = (stats['æ¨¡å‹æ€»å‚æ•°'] * 2 * num_tasks) / (1024**3)
            lora_storage_mb = (stats['LoRAå‚æ•°'] * 2 * num_tasks) / (1024**2)
            
            savings = full_storage_gb / (lora_storage_mb / 1024)
            
            print(f"{model_name:<12} {full_storage_gb:>15.1f} GB    {lora_storage_mb:>15.1f} MB    {savings:>10.1f}x")
    
    def visualize_comparison(self, model_name: str = '7B'):
        """å¯è§†åŒ–å‚æ•°å¯¹æ¯”"""
        try:
            stats = self.calculate_lora_params(model_name)
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
            
            # å·¦å›¾ï¼šå‚æ•°é‡å¯¹æ¯”ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
            categories = ['å…¨é‡å¾®è°ƒ', 'LoRA']
            params = [stats['æ¨¡å‹æ€»å‚æ•°'], stats['LoRAå‚æ•°']]
            colors = ['#ef4444', '#10b981']
            
            bars1 = ax1.bar(categories, params, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
            ax1.set_yscale('log')
            ax1.set_ylabel('å‚æ•°é‡ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰', fontsize=12, fontweight='bold')
            ax1.set_title(f'{model_name}æ¨¡å‹å‚æ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax1.grid(axis='y', alpha=0.3, linestyle='--')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (bar, value) in enumerate(zip(bars1, params)):
                height = bar.get_height()
                label = f'{value:,.0f}\n({value/1e9:.1f}B)' if i == 0 else f'{value:,.0f}\n({value/1e6:.1f}M)'
                ax1.text(bar.get_x() + bar.get_width()/2., height,
                        label, ha='center', va='bottom', fontsize=10, fontweight='bold')
            
            # å³å›¾ï¼šå‚æ•°å æ¯”é¥¼å›¾
            sizes = [stats['æ¨¡å‹æ€»å‚æ•°'] - stats['LoRAå‚æ•°'], stats['LoRAå‚æ•°']]
            labels = [
                f'å†»ç»“å‚æ•°\n{(sizes[0]/1e9):.2f}B\n({sizes[0]/stats["æ¨¡å‹æ€»å‚æ•°"]*100:.2f}%)',
                f'LoRAå‚æ•°\n{(sizes[1]/1e6):.2f}M\n({sizes[1]/stats["æ¨¡å‹æ€»å‚æ•°"]*100:.3f}%)'
            ]
            colors_pie = ['#94a3b8', '#10b981']
            explode = (0, 0.1)  # çªå‡ºLoRAéƒ¨åˆ†
            
            wedges, texts, autotexts = ax2.pie(sizes, labels=labels, colors=colors_pie,
                                                autopct='', explode=explode,
                                                shadow=True, startangle=90,
                                                textprops={'fontsize': 11, 'fontweight': 'bold'})
            
            ax2.set_title(f'{model_name}æ¨¡å‹å‚æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
            
            plt.tight_layout()
            plt.savefig('code/part2/parameter_comparison.png', dpi=150, bbox_inches='tight')
            print(f"\nğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: code/part2/parameter_comparison.png")
            plt.show()
            
        except ImportError:
            print("\nâš ï¸  matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–")
            print("   å®‰è£…å‘½ä»¤: pip install matplotlib")
    
    def show_memory_breakdown(self, model_name: str = '7B'):
        """å±•ç¤ºå†…å­˜ä½¿ç”¨è¯¦ç»†åˆ†è§£"""
        print(f"\n{'='*80}")
        print(f"ğŸ’» {model_name}æ¨¡å‹è®­ç»ƒå†…å­˜éœ€æ±‚åˆ†è§£")
        print(f"{'='*80}")
        
        stats = self.calculate_lora_params(model_name)
        total_params = stats['æ¨¡å‹æ€»å‚æ•°']
        lora_params = stats['LoRAå‚æ•°']
        
        print(f"\nã€å…¨é‡å¾®è°ƒã€‘")
        print(f"-"*80)
        
        # FP32 è®­ç»ƒ
        model_memory_fp32 = (total_params * 4) / (1024**3)
        optimizer_memory = model_memory_fp32 * 2  # Adam states
        gradients_memory = model_memory_fp32
        activations_memory = model_memory_fp32 * 0.5
        
        total_full_ft = model_memory_fp32 + optimizer_memory + gradients_memory + activations_memory
        
        print(f"  æ¨¡å‹å‚æ•°ï¼ˆFP32ï¼‰:    {model_memory_fp32:>10.2f} GB")
        print(f"  ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamï¼‰:  {optimizer_memory:>10.2f} GB")
        print(f"  æ¢¯åº¦:               {gradients_memory:>10.2f} GB")
        print(f"  æ¿€æ´»å€¼ï¼ˆä¼°ç®—ï¼‰:      {activations_memory:>10.2f} GB")
        print(f"  {'-'*50}")
        print(f"  æ€»è®¡:               {total_full_ft:>10.2f} GB")
        
        print(f"\nã€LoRAå¾®è°ƒã€‘")
        print(f"-"*80)
        
        # åŸºåº§æ¨¡å‹ï¼ˆFP16ï¼Œå†»ç»“ï¼‰
        base_model_fp16 = (total_params * 2) / (1024**3)
        # LoRAå‚æ•°ï¼ˆFP32ï¼Œè®­ç»ƒï¼‰
        lora_memory_fp32 = (lora_params * 4) / (1024**3)
        lora_optimizer = lora_memory_fp32 * 2
        lora_gradients = lora_memory_fp32
        lora_activations = activations_memory  # æ¿€æ´»å€¼ç›¸åŒ
        
        total_lora = base_model_fp16 + lora_memory_fp32 + lora_optimizer + lora_gradients + lora_activations
        
        print(f"  åŸºåº§æ¨¡å‹ï¼ˆFP16ï¼Œå†»ç»“ï¼‰: {base_model_fp16:>10.2f} GB")
        print(f"  LoRAå‚æ•°ï¼ˆFP32ï¼‰:       {lora_memory_fp32:>10.2f} GB")
        print(f"  ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamï¼‰:     {lora_optimizer:>10.2f} GB")
        print(f"  æ¢¯åº¦:                  {lora_gradients:>10.2f} GB")
        print(f"  æ¿€æ´»å€¼ï¼ˆä¼°ç®—ï¼‰:         {lora_activations:>10.2f} GB")
        print(f"  {'-'*50}")
        print(f"  æ€»è®¡:                  {total_lora:>10.2f} GB")
        
        print(f"\n{'='*80}")
        print(f"ğŸ’¡ å†…å­˜èŠ‚çœ: {total_full_ft - total_lora:.2f} GB ({(total_full_ft - total_lora)/total_full_ft*100:.1f}%)")
        print(f"{'='*80}")
        
        # GPUå»ºè®®
        print(f"\nğŸ–¥ï¸  GPUå»ºè®®:")
        if total_lora <= 24:
            print(f"  LoRA: å•å¼ RTX 3090/4090 (24GB) å³å¯è®­ç»ƒ")
        elif total_lora <= 40:
            print(f"  LoRA: å•å¼ A100 (40GB) å¯ä»¥è®­ç»ƒ")
        else:
            print(f"  LoRA: éœ€è¦A100 (80GB) æˆ–å¤šå¡")
        
        if total_full_ft <= 80:
            print(f"  å…¨é‡å¾®è°ƒ: è‡³å°‘éœ€è¦A100 (80GB)")
        else:
            print(f"  å…¨é‡å¾®è°ƒ: éœ€è¦å¤šå¼ A100 (80GB)")


def main():
    """ä¸»å‡½æ•°"""
    comparator = ParameterComparison()
    
    # è¯¦ç»†å¯¹æ¯”
    comparator.print_detailed_comparison()
    
    # å­˜å‚¨èŠ‚çœ
    comparator.calculate_storage_savings()
    
    # å†…å­˜åˆ†è§£ï¼ˆä»¥7Bä¸ºä¾‹ï¼‰
    comparator.show_memory_breakdown('7B')
    
    # å¯è§†åŒ–å¯¹æ¯”
    print("\n" + "="*80)
    comparator.visualize_comparison('7B')
    
    print("\nğŸ“ ä»£ç ä½ç½®: code/part2/03_parameter_comparison.py")
    print("\nğŸ’¡ æç¤ºï¼šä¿®æ”¹ model_name å‚æ•°å¯ä»¥æŸ¥çœ‹ä¸åŒæ¨¡å‹è§„æ¨¡çš„å¯¹æ¯”")


if __name__ == "__main__":
    main()

