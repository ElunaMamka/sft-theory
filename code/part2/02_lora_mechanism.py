"""
æ¼”ç¤ºä»£ç ï¼šLoRAå·¥ä½œæœºåˆ¶
æ–‡ä»¶ä½ç½®ï¼šcode/part2/02_lora_mechanism.py

è¯¦ç»†æ¼”ç¤ºLoRAçš„ä¸¤ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼š
1. å†»ç»“é¢„è®­ç»ƒæ¨¡å‹
2. æ·»åŠ ä½ç§©é€‚é…å™¨
"""

import numpy as np
from typing import Tuple, List


class LoRALayer:
    """LoRAå±‚çš„ç®€åŒ–å®ç°"""
    
    def __init__(self, input_dim: int, output_dim: int, rank: int = 8, alpha: float = 16):
        """
        åˆå§‹åŒ–LoRAå±‚
        
        Args:
            input_dim: è¾“å…¥ç»´åº¦
            output_dim: è¾“å‡ºç»´åº¦
            rank: LoRAç§©ï¼ˆæ§åˆ¶é€‚é…å™¨å¤§å°ï¼‰
            alpha: LoRAç¼©æ”¾å› å­
        """
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        
        # LoRAçŸ©é˜µï¼šW = B @ A
        self.lora_A = np.random.randn(input_dim, rank) * 0.01
        self.lora_B = np.zeros((rank, output_dim))
        
        # åŸå§‹å†»ç»“æƒé‡ï¼ˆä¸è®­ç»ƒï¼‰
        self.frozen_weight = np.random.randn(input_dim, output_dim) * 0.1
        
    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ shape (batch, input_dim)
            
        Returns:
            è¾“å‡º shape (batch, output_dim)
        """
        # åŸå§‹æƒé‡çš„è¾“å‡ºï¼ˆå†»ç»“ï¼‰
        original_output = x @ self.frozen_weight
        
        # LoRAé€‚é…å™¨çš„è¾“å‡ºï¼ˆå¯è®­ç»ƒï¼‰
        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling
        
        # åˆå¹¶è¾“å‡º
        return original_output + lora_output
    
    def count_parameters(self) -> dict:
        """ç»Ÿè®¡å‚æ•°é‡"""
        frozen_params = self.input_dim * self.output_dim
        trainable_params = self.input_dim * self.rank + self.rank * self.output_dim
        
        return {
            "å†»ç»“å‚æ•°": frozen_params,
            "å¯è®­ç»ƒå‚æ•°ï¼ˆLoRAï¼‰": trainable_params,
            "å‚æ•°æ€»é‡": frozen_params + trainable_params,
            "è®­ç»ƒå‚æ•°æ¯”ä¾‹": f"{trainable_params / (frozen_params + trainable_params) * 100:.3f}%"
        }


def demonstrate_lora_mechanism():
    """æ¼”ç¤ºLoRAæœºåˆ¶"""
    
    print("="*80)
    print("ğŸ”Œ LoRA (Low-Rank Adaptation) å·¥ä½œæœºåˆ¶æ¼”ç¤º")
    print("="*80)
    
    # åˆ›å»ºä¸€ä¸ªLoRAå±‚ï¼ˆæ¨¡æ‹Ÿæ³¨æ„åŠ›å±‚ï¼‰
    input_dim = 4096  # å…¸å‹çš„éšè—å±‚ç»´åº¦
    output_dim = 4096
    rank = 8  # LoRAç§©
    
    print(f"\nğŸ“ å±‚é…ç½®ï¼š")
    print(f"  è¾“å…¥ç»´åº¦: {input_dim}")
    print(f"  è¾“å‡ºç»´åº¦: {output_dim}")
    print(f"  LoRAç§© (r): {rank}")
    
    lora_layer = LoRALayer(input_dim, output_dim, rank)
    
    # æ­¥éª¤1ï¼šå±•ç¤ºå†»ç»“æœºåˆ¶
    print(f"\n{'='*80}")
    print("æ­¥éª¤ 1: å†»ç»“é¢„è®­ç»ƒæ¨¡å‹æƒé‡ â„ï¸")
    print(f"{'='*80}")
    
    print(f"\nğŸ§Š åŸå§‹æƒé‡çŸ©é˜µ W:")
    print(f"  â€¢ å½¢çŠ¶: ({input_dim}, {output_dim})")
    print(f"  â€¢ å‚æ•°é‡: {input_dim * output_dim:,}")
    print(f"  â€¢ çŠ¶æ€: ğŸ”’ å®Œå…¨å†»ç»“ï¼ˆrequires_grad=Falseï¼‰")
    print(f"  â€¢ ä½œç”¨: ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ï¼Œé˜²æ­¢é—å¿˜")
    
    # æ­¥éª¤2ï¼šå±•ç¤ºLoRAé€‚é…å™¨
    print(f"\n{'='*80}")
    print("æ­¥éª¤ 2: æ·»åŠ ä½ç§©é€‚é…å™¨çŸ©é˜µ ğŸ”Œ")
    print(f"{'='*80}")
    
    print(f"\nğŸ¯ LoRAåˆ†è§£ï¼šÎ”W = B @ A")
    print(f"\n  çŸ©é˜µ A:")
    print(f"    â€¢ å½¢çŠ¶: ({input_dim}, {rank})")
    print(f"    â€¢ å‚æ•°é‡: {input_dim * rank:,}")
    print(f"    â€¢ åˆå§‹åŒ–: é«˜æ–¯éšæœºæ•°")
    
    print(f"\n  çŸ©é˜µ B:")
    print(f"    â€¢ å½¢çŠ¶: ({rank}, {output_dim})")
    print(f"    â€¢ å‚æ•°é‡: {rank * output_dim:,}")
    print(f"    â€¢ åˆå§‹åŒ–: å…¨é›¶ï¼ˆè®­ç»ƒå¼€å§‹æ—¶Î”W=0ï¼‰")
    
    # å‚æ•°ç»Ÿè®¡
    print(f"\n{'='*80}")
    print("ğŸ“Š å‚æ•°ç»Ÿè®¡å¯¹æ¯”")
    print(f"{'='*80}")
    
    stats = lora_layer.count_parameters()
    for key, value in stats.items():
        print(f"  {key}: {value if isinstance(value, str) else f'{value:,}'}")
    
    # å¯è§†åŒ–åˆ†è§£è¿‡ç¨‹
    print(f"\n{'='*80}")
    print("ğŸ¨ LoRAçš„æ•°å­¦æœ¬è´¨")
    print(f"{'='*80}")
    
    print("""
åŸå§‹å…¨é‡å¾®è°ƒï¼š
  y = x @ W_new
  éœ€è¦æ›´æ–°: W_new (æ‰€æœ‰å‚æ•°)

LoRAæ–¹æ³•ï¼š
  y = x @ (W_frozen + Î”W)
  y = x @ (W_frozen + B @ A)
  
  å…¶ä¸­:
  â€¢ W_frozen: å†»ç»“çš„é¢„è®­ç»ƒæƒé‡
  â€¢ Î”W = B @ A: ä½ç§©æ›´æ–°ï¼ˆä»…è®­ç»ƒBå’ŒAï¼‰
  â€¢ rank << min(input_dim, output_dim)
    """)
    
    # å‰å‘ä¼ æ’­æ¼”ç¤º
    print(f"\n{'='*80}")
    print("ğŸ”„ å‰å‘ä¼ æ’­æµç¨‹")
    print(f"{'='*80}")
    
    batch_size = 2
    x = np.random.randn(batch_size, input_dim)
    
    print(f"\nè¾“å…¥ x: shape {x.shape}")
    print(f"  â†“")
    print(f"1ï¸âƒ£  x @ W_frozen  â†’  åŸå§‹æ¨¡å‹è¾“å‡º")
    print(f"  â†“")
    print(f"2ï¸âƒ£  x @ A @ B  â†’  LoRAé€‚é…å™¨è¾“å‡º")
    print(f"  â†“")
    print(f"3ï¸âƒ£  åŸå§‹è¾“å‡º + LoRAè¾“å‡º  â†’  æœ€ç»ˆè¾“å‡º")
    
    output = lora_layer.forward(x)
    print(f"\nè¾“å‡º y: shape {output.shape}")
    
    # LoRAçš„ä¼˜åŠ¿
    print(f"\n{'='*80}")
    print("âœ¨ LoRAçš„æ ¸å¿ƒä¼˜åŠ¿")
    print(f"{'='*80}")
    
    trainable_ratio = stats["å¯è®­ç»ƒå‚æ•°ï¼ˆLoRAï¼‰"] / stats["å‚æ•°æ€»é‡"] * 100
    
    print(f"""
1. å‚æ•°æ•ˆç‡ ğŸ’¾
   â€¢ åªè®­ç»ƒ {trainable_ratio:.2f}% çš„å‚æ•°
   â€¢ å†…å­˜å ç”¨å¤§å¹…å‡å°‘
   â€¢ è®­ç»ƒé€Ÿåº¦æ˜¾è‘—æå‡

2. é˜²æ­¢é—å¿˜ ğŸ›¡ï¸
   â€¢ åŸå§‹æƒé‡å®Œå…¨å†»ç»“
   â€¢ é¢„è®­ç»ƒçŸ¥è¯†å¾—åˆ°ä¿æŠ¤
   â€¢ ç¾éš¾æ€§é—å¿˜é£é™©æä½

3. æ¨¡å—åŒ–éƒ¨ç½² ğŸ”Œ
   â€¢ LoRAé€‚é…å™¨å¯ä»¥å•ç‹¬ä¿å­˜ï¼ˆä»…å‡ MBï¼‰
   â€¢ æ”¯æŒåŠ¨æ€åŠ è½½/å¸è½½
   â€¢ ä¸€ä¸ªåŸºåº§æ¨¡å‹ + å¤šä¸ªé€‚é…å™¨
    """)


def compare_with_full_finetuning():
    """ä¸å…¨é‡å¾®è°ƒå¯¹æ¯”"""
    
    print(f"\n{'='*80}")
    print("âš–ï¸  LoRA vs å…¨é‡å¾®è°ƒ - å…·ä½“æ•°å­—å¯¹æ¯”")
    print(f"{'='*80}")
    
    # ä»¥7Bæ¨¡å‹ä¸ºä¾‹
    model_size = 7_000_000_000
    lora_rank = 8
    num_lora_layers = 32  # å‡è®¾32ä¸ªattentionå±‚åº”ç”¨LoRA
    dim = 4096
    
    # å…¨é‡å¾®è°ƒ
    full_params = model_size
    full_storage_gb = (model_size * 2) / (1024**3)  # FP16
    
    # LoRA
    lora_params_per_layer = dim * lora_rank * 2  # Aå’ŒBçŸ©é˜µ
    total_lora_params = lora_params_per_layer * num_lora_layers
    lora_storage_mb = (total_lora_params * 2) / (1024**2)  # FP16
    
    print(f"\næ¨¡å‹è§„æ¨¡: 7Bå‚æ•°")
    print(f"\nå…¨é‡å¾®è°ƒ:")
    print(f"  â€¢ å¯è®­ç»ƒå‚æ•°: {full_params:,}")
    print(f"  â€¢ å­˜å‚¨éœ€æ±‚: {full_storage_gb:.2f} GB")
    
    print(f"\nLoRA (r={lora_rank}):")
    print(f"  â€¢ å¯è®­ç»ƒå‚æ•°: {total_lora_params:,}")
    print(f"  â€¢ å­˜å‚¨éœ€æ±‚: {lora_storage_mb:.2f} MB")
    
    print(f"\nğŸ“‰ å‡å°‘å€æ•°:")
    print(f"  â€¢ å‚æ•°é‡: {full_params / total_lora_params:.1f}x")
    print(f"  â€¢ å­˜å‚¨: {full_storage_gb * 1024 / lora_storage_mb:.1f}x")
    
    print("\nğŸ“ ä»£ç ä½ç½®: code/part2/02_lora_mechanism.py")


if __name__ == "__main__":
    demonstrate_lora_mechanism()
    compare_with_full_finetuning()

