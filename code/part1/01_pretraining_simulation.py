"""
æ¼”ç¤ºä»£ç ï¼šé¢„è®­ç»ƒè¿‡ç¨‹æ¨¡æ‹Ÿ
æ–‡ä»¶ä½ç½®ï¼šcode/part1/01_pretraining_simulation.py

è¿™ä¸ªè„šæœ¬æ¨¡æ‹Ÿäº†é¢„è®­ç»ƒçš„åŸºæœ¬è¿‡ç¨‹ï¼š
1. åŠ è½½å¤§é‡æ–‡æœ¬æ•°æ®
2. é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
3. è®¡ç®—æŸå¤±å¹¶æ›´æ–°æ¨¡å‹
"""

import random
from typing import List, Dict

class SimplePretraining:
    """ç®€åŒ–çš„é¢„è®­ç»ƒæ¨¡æ‹Ÿå™¨"""
    
    def __init__(self, vocab_size: int = 1000):
        """
        åˆå§‹åŒ–é¢„è®­ç»ƒæ¨¡å‹
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
        """
        self.vocab_size = vocab_size
        # ç®€åŒ–çš„æ¨¡å‹å‚æ•°ï¼ˆå®é™…æ˜¯æ•°åäº¿å‚æ•°ï¼‰
        self.model_params = [random.random() for _ in range(100)]
        self.training_loss = []
        
    def load_text_data(self) -> List[str]:
        """
        æ¨¡æ‹ŸåŠ è½½æµ·é‡æ–‡æœ¬æ•°æ®
        
        Returns:
            æ–‡æœ¬åˆ—è¡¨ï¼ˆå®é™…åº”è¯¥æ˜¯ä¸‡äº¿çº§tokensï¼‰
        """
        # æ¨¡æ‹Ÿäº’è”ç½‘æ–‡æœ¬æ•°æ®
        sample_texts = [
            "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
            "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ",
            "é¢„è®­ç»ƒæ¨¡å‹éœ€è¦å¤§é‡æ•°æ®",
            "è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨å¹¿æ³›"
        ]
        
        print(f"ğŸ“š åŠ è½½äº† {len(sample_texts)} æ¡æ–‡æœ¬ï¼ˆå®é™…åº”è¯¥æ˜¯ä¸‡äº¿tokensï¼‰")
        return sample_texts
    
    def tokenize(self, text: str) -> List[int]:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºtokenåºåˆ—
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            token IDåˆ—è¡¨
        """
        # ç®€åŒ–çš„åˆ†è¯ï¼ˆå®é™…ä½¿ç”¨BPEç­‰ç®—æ³•ï¼‰
        tokens = [hash(word) % self.vocab_size for word in text.split()]
        return tokens
    
    def predict_next_token(self, context_tokens: List[int]) -> Dict[int, float]:
        """
        é¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒ
        
        Args:
            context_tokens: ä¸Šä¸‹æ–‡tokens
            
        Returns:
            tokenåˆ°æ¦‚ç‡çš„æ˜ å°„
        """
        # ç®€åŒ–çš„é¢„æµ‹ï¼ˆå®é™…ä½¿ç”¨Transformerï¼‰
        predictions = {}
        for i in range(min(5, self.vocab_size)):
            predictions[i] = random.random()
        
        # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡
        total = sum(predictions.values())
        predictions = {k: v/total for k, v in predictions.items()}
        
        return predictions
    
    def calculate_loss(self, predicted: Dict[int, float], actual: int) -> float:
        """
        è®¡ç®—äº¤å‰ç†µæŸå¤±
        
        Args:
            predicted: é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒ
            actual: å®é™…çš„ä¸‹ä¸€ä¸ªtoken
            
        Returns:
            æŸå¤±å€¼
        """
        # ç®€åŒ–çš„æŸå¤±è®¡ç®—
        actual_prob = predicted.get(actual, 0.01)
        loss = -1 * (actual_prob if actual_prob > 0 else 0.01)
        return abs(loss)
    
    def train_epoch(self, texts: List[str]) -> float:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            texts: è®­ç»ƒæ–‡æœ¬
            
        Returns:
            å¹³å‡æŸå¤±
        """
        total_loss = 0
        samples = 0
        
        for text in texts:
            tokens = self.tokenize(text)
            
            # å¯¹æ¯ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
            for i in range(len(tokens) - 1):
                context = tokens[:i+1]
                next_token = tokens[i+1]
                
                # é¢„æµ‹
                predictions = self.predict_next_token(context)
                
                # è®¡ç®—æŸå¤±
                loss = self.calculate_loss(predictions, next_token)
                total_loss += loss
                samples += 1
                
                # æ¨¡æ‹Ÿå‚æ•°æ›´æ–°ï¼ˆå®é™…ä½¿ç”¨åå‘ä¼ æ’­ï¼‰
                self.model_params[0] = self.model_params[0] * 0.999
        
        avg_loss = total_loss / samples if samples > 0 else 0
        return avg_loss
    
    def pretrain(self, num_epochs: int = 10):
        """
        æ‰§è¡Œå®Œæ•´çš„é¢„è®­ç»ƒè¿‡ç¨‹
        
        Args:
            num_epochs: è®­ç»ƒè½®æ•°ï¼ˆå®é™…å¯èƒ½éœ€è¦éå†æ•°æ®å¤šæ¬¡ï¼‰
        """
        print("\nğŸš€ å¼€å§‹é¢„è®­ç»ƒ...")
        print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {self.vocab_size:,}")
        print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {num_epochs}")
        print("-" * 60)
        
        # åŠ è½½æ•°æ®
        texts = self.load_text_data()
        
        # è®­ç»ƒå¤šä¸ªepoch
        for epoch in range(num_epochs):
            avg_loss = self.train_epoch(texts)
            self.training_loss.append(avg_loss)
            
            print(f"Epoch {epoch+1:2d}/{num_epochs} | Loss: {avg_loss:.4f}")
        
        print("-" * 60)
        print("âœ… é¢„è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“‰ æœ€ç»ˆæŸå¤±: {self.training_loss[-1]:.4f}")
        print(f"ğŸ“ˆ æŸå¤±ä¸‹é™: {(self.training_loss[0] - self.training_loss[-1]):.4f}")


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºé¢„è®­ç»ƒè¿‡ç¨‹"""
    
    print("="* 60)
    print("ğŸ“ é¢„è®­ç»ƒè¿‡ç¨‹æ¼”ç¤º")
    print("="* 60)
    
    # åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹
    model = SimplePretraining(vocab_size=1000)
    
    # æ‰§è¡Œé¢„è®­ç»ƒ
    model.pretrain(num_epochs=10)
    
    print("\nğŸ’¡ å…³é”®è¦ç‚¹ï¼š")
    print("1. é¢„è®­ç»ƒä½¿ç”¨æµ·é‡æœªæ ‡æ³¨æ–‡æœ¬")
    print("2. æ ¸å¿ƒä»»åŠ¡æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯")
    print("3. é€šè¿‡è‡ªç›‘ç£å­¦ä¹ è·å¾—è¯­è¨€ç†è§£èƒ½åŠ›")
    print("4. æŸå¤±æŒç»­ä¸‹é™è¡¨ç¤ºæ¨¡å‹åœ¨å­¦ä¹ ")
    
    print("\nğŸ“ ä»£ç ä½ç½®: code/part1/01_pretraining_simulation.py")


if __name__ == "__main__":
    main()

