"""
4.1 ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹ - LoRAå¾®è°ƒè®­ç»ƒè„šæœ¬
å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°
"""

import os
import sys
import torch
from datasets import load_from_disk
from transformers import (
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model.model_config import ModelConfig, LoRAConfig, TrainingConfig
from model.model_utils import (
    load_tokenizer,
    load_model_for_training,
    format_instruction,
)


class TCMTrainer:
    """ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹è®­ç»ƒå™¨"""
    
    def __init__(self, 
                 model_cfg: ModelConfig,
                 lora_cfg: LoRAConfig,
                 train_cfg: TrainingConfig):
        self.model_cfg = model_cfg
        self.lora_cfg = lora_cfg
        self.train_cfg = train_cfg
        
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.eval_dataset = None
    
    def setup(self):
        """åˆå§‹åŒ–è®¾ç½®"""
        print("="*80)
        print("ğŸš€ ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹ - LoRAå¾®è°ƒè®­ç»ƒ")
        print("="*80)
        
        # 1. åŠ è½½tokenizer
        self.tokenizer = load_tokenizer(
            self.model_cfg.model_name_or_path,
            trust_remote_code=self.model_cfg.trust_remote_code,
            use_fast=self.model_cfg.use_fast_tokenizer,
        )
        
        # 2. åŠ è½½å¹¶é…ç½®æ¨¡å‹
        self.model = load_model_for_training(
            model_name_or_path=self.model_cfg.model_name_or_path,
            load_in_4bit=self.model_cfg.load_in_4bit,
            lora_r=self.lora_cfg.r,
            lora_alpha=self.lora_cfg.lora_alpha,
            lora_dropout=self.lora_cfg.lora_dropout,
        )
        
        print(f"\nâœ… åˆå§‹åŒ–å®Œæˆï¼")
    
    def load_data(self, train_data_path: str, eval_data_path: str):
        """åŠ è½½æ•°æ®é›†"""
        print("\n" + "="*80)
        print("ğŸ“š åŠ è½½è®­ç»ƒæ•°æ®")
        print("="*80)
        
        # åŠ è½½HuggingFaceæ ¼å¼çš„æ•°æ®
        self.train_dataset = load_from_disk(train_data_path)
        self.eval_dataset = load_from_disk(eval_data_path)
        
        print(f"  è®­ç»ƒé›†: {len(self.train_dataset)} æ¡")
        print(f"  éªŒè¯é›†: {len(self.eval_dataset)} æ¡")
        
        # æ•°æ®é¢„å¤„ç†
        self.train_dataset = self.train_dataset.map(
            self.tokenize_function,
            remove_columns=self.train_dataset.column_names,
            desc="Tokenizing train dataset",
        )
        
        self.eval_dataset = self.eval_dataset.map(
            self.tokenize_function,
            remove_columns=self.eval_dataset.column_names,
            desc="Tokenizing eval dataset",
        )
        
        print(f"  âœ… æ•°æ®åŠ è½½å’Œtokenizationå®Œæˆ")
    
    def tokenize_function(self, examples):
        """Tokenizationå‡½æ•°"""
        # ä½¿ç”¨é¢„å…ˆç”Ÿæˆçš„textå­—æ®µï¼ˆå·²åŒ…å«instruction+input+outputï¼‰
        outputs = self.tokenizer(
            examples['text'],
            truncation=True,
            max_length=self.model_cfg.max_length,
            padding=False,  # åŠ¨æ€padding
            return_tensors=None,
        )
        
        # è®¾ç½®labelsï¼ˆç”¨äºè®¡ç®—lossï¼‰
        outputs['labels'] = outputs['input_ids'].copy()
        
        return outputs
    
    def create_trainer(self):
        """åˆ›å»ºTrainer"""
        print("\n" + "="*80)
        print("âš™ï¸ é…ç½®è®­ç»ƒå™¨")
        print("="*80)
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.train_cfg.output_dir,
            num_train_epochs=self.train_cfg.num_train_epochs,
            per_device_train_batch_size=self.train_cfg.per_device_train_batch_size,
            per_device_eval_batch_size=self.train_cfg.per_device_eval_batch_size,
            gradient_accumulation_steps=self.train_cfg.gradient_accumulation_steps,
            learning_rate=self.train_cfg.learning_rate,
            lr_scheduler_type=self.train_cfg.lr_scheduler_type,
            warmup_ratio=self.train_cfg.warmup_ratio,
            weight_decay=self.train_cfg.weight_decay,
            max_grad_norm=self.train_cfg.max_grad_norm,
            logging_steps=self.train_cfg.logging_steps,
            logging_dir=self.train_cfg.logging_dir,
            save_strategy=self.train_cfg.save_strategy,
            save_steps=self.train_cfg.save_steps,
            save_total_limit=self.train_cfg.save_total_limit,
            evaluation_strategy=self.train_cfg.evaluation_strategy,
            eval_steps=self.train_cfg.eval_steps,
            fp16=self.train_cfg.fp16,
            bf16=self.train_cfg.bf16,
            optim=self.train_cfg.optim,
            seed=self.train_cfg.seed,
            dataloader_num_workers=self.train_cfg.dataloader_num_workers,
            remove_unused_columns=self.train_cfg.remove_unused_columns,
            report_to=self.train_cfg.report_to,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
        )
        
        # Data collatorï¼ˆç”¨äºåŠ¨æ€paddingï¼‰
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # Causal LMä¸éœ€è¦MLM
        )
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
        )
        
        print(f"  âœ… Traineré…ç½®å®Œæˆ")
        print(f"     å®é™…batch size: {self.train_cfg.per_device_train_batch_size * self.train_cfg.gradient_accumulation_steps}")
        print(f"     æ€»è®­ç»ƒæ­¥æ•°: {len(self.train_dataset) // (self.train_cfg.per_device_train_batch_size * self.train_cfg.gradient_accumulation_steps) * self.train_cfg.num_train_epochs}")
        
        return trainer
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        print("\n" + "="*80)
        print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ")
        print("="*80)
        
        # åˆ›å»ºtrainer
        trainer = self.create_trainer()
        
        # è®­ç»ƒ
        train_result = trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        trainer.save_model()
        self.tokenizer.save_pretrained(self.train_cfg.output_dir)
        
        # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
        print(f"   æ¨¡å‹ä¿å­˜åˆ°: {self.train_cfg.output_dir}")
        print(f"   è®­ç»ƒæŸå¤±: {metrics['train_loss']:.4f}")
        
        return trainer


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # é…ç½®
    model_cfg = ModelConfig(
        model_name_or_path="baichuan-inc/Baichuan2-7B-Base",
        max_length=2048,
        load_in_4bit=True,
    )
    
    lora_cfg = LoRAConfig(
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
    )
    
    train_cfg = TrainingConfig(
        output_dir="./output/tcm_sft_lora",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        logging_steps=10,
        save_steps=100,
        eval_steps=100,
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = TCMTrainer(model_cfg, lora_cfg, train_cfg)
    
    # åˆå§‹åŒ–
    trainer.setup()
    
    # åŠ è½½æ•°æ®
    trainer.load_data(
        train_data_path="../data/processed/train",
        eval_data_path="../data/processed/val",
    )
    
    # è®­ç»ƒ
    trainer.train()
    
    print("\n" + "="*80)
    print("ğŸ‰ è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")
    print("="*80)
    print("\nåç»­æ­¥éª¤:")
    print("  1. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: tensorboard --logdir ./logs")
    print("  2. æµ‹è¯•æ¨¡å‹: python ../inference/test_model.py")
    print("  3. äº¤äº’å¼æ¼”ç¤º: python ../inference/interactive_demo.py")


if __name__ == "__main__":
    # æ£€æŸ¥ç¯å¢ƒ
    print("ç¯å¢ƒæ£€æŸ¥:")
    print(f"  Python: {sys.version}")
    print(f"  PyTorch: {torch.__version__}")
    print(f"  CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"  GPUåç§°: {torch.cuda.get_device_name(0)}")
    print()
    
    # è¿è¡Œè®­ç»ƒ
    main()

