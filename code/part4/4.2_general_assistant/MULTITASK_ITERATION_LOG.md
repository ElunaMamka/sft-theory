# 通用对话助手 - 多任务SFT迭代记录

> 这是多任务SFT的真实迭代记录。
> 
> 与单任务(4.1中医)的主要区别：
> - 🆚 **任务平衡** 是核心挑战（不只是数据量，还有任务冲突）
> - 🆚 **任务串扰** 是特有问题（任务间相互干扰）
> - 🆚 **迭代周期更长**（2-3周 vs 1周）

---

## 📊 与单任务SFT的关键差异

| 维度 | 单任务SFT (4.1中医) | 多任务SFT (4.2通用) |
|------|------------------|-----------------|
| **主要挑战** | 数据质量、专业深度 | 任务平衡、任务冲突 |
| **数据量** | 150-200条 | 1500-2000条 |
| **迭代周期** | 1周 | 2-3周 |
| **核心问题** | 复读、知识缺失 | 任务串扰、性能不均 |
| **训练策略** | LoRA微调 | LoRA或全参数 |
| **测试复杂度** | 测试一个领域 | 测试10+个任务 |

---

## 🔴 Version 0.1 - 初始尝试（Day 1-2）

### 配置
```yaml
任务类型: 10种
总数据量: 1200条
分布: 严重不均（问答600, 翻译50...）
模型: Qwen-7B + LoRA(r=8)
训练: 3 epochs
```

### 测试结果

**问答任务：** ✅ 85%准确率（数据多）
**代码生成：** ✅ 80%准确率（数据多）
**翻译任务：** ❌ 38%准确率（数据少）
**逻辑推理：** ❌ 45%准确率（数据少+难度高）

### ❌ 核心问题

<div class="problem">
<h3>问题1：任务性能严重不均</h3>

数据多的任务性能好，数据少的任务性能差。

最好(问答85%) vs 最差(翻译38%)，相差47%！
</div>

<div class="problem">
<h3>问题2：高频任务'污染'低频任务</h3>

**案例：**
```
任务: 翻译
输入: Translate "I love AI" to Chinese
期望: 我爱人工智能
实际: AI是一种...（开始回答问答题！）
```

模型把翻译任务当成了问答任务！
</div>

### 💡 教训

> ⚠️ **多任务的挑战远大于单任务！**
>
> 不是简单的"把多种数据混在一起训练"
> 需要精心的任务平衡和边界设计

---

## 🟡 Version 0.2 - 任务平衡（Day 3-5）

### 改进措施

1. **上采样低频任务**
   ```python
   # 每个任务至少100条
   if len(task_data) < 100:
       task_data = task_data * (100 // len(task_data) + 1)
   ```

2. **下采样高频任务**
   ```python
   # 每个任务最多150条
   if len(task_data) > 150:
       task_data = random.sample(task_data, 150)
   ```

3. **最终分布**
   ```
   每个任务: 100-150条
   总数据: 约1300条
   分布: 基本均衡
   ```

### 训练配置
```yaml
数据: 1300条（平衡后）
训练: 3 epochs
其他: 同v0.1
```

### 测试结果

**翻译：** 38% → 62% ✅（提升24%！）
**逻辑推理：** 45% → 58% ✅（提升13%）
**问答：** 85% → 82% ⚠️（轻微下降）

### ❌ 新问题

<div class="problem">
<h3>问题：任务串扰更严重了！</h3>

**案例1：问答任务**
```
输入: 什么是机器学习？
期望: ML是一种AI技术...(简洁)
实际: 以下是5种机器学习方法：
      1. 监督学习
      2. 无监督学习
      ...
```
❌ 被头脑风暴任务影响，总想列举多个

**案例2：代码任务**
```
输入: 实现快速排序
期望: def quicksort...
实际: 根据您的需求，我提供3种方案：
      方案1: 快速排序
      方案2: 归并排序
      方案3: 堆排序
      请问您需要哪种？
```
❌ 被对话任务影响，反问用户
</div>

### 🔍 问题分析

数据平衡后，任务串扰反而加剧了！

**原因：**
- 头脑风暴任务教会模型"多个选项是好的"
- 对话任务教会模型"反问是好的"
- 这些模式泛化到了所有任务

**根本问题：**
不同任务对模型行为的要求是**矛盾的**！

---

## 🟡 Version 0.3 - 强化任务边界（Day 6-8）

### 改进措施

1. **添加任务标签**
   ```python
   # 之前
   instruction = "请回答问题"
   
   # 现在
   instruction = "[任务: 问答] 请给出简洁准确的答案。不要列举多个选项。"
   ```

2. **任务特定约束**
   ```python
   task_constraints = {
       'qa': '只回答一个答案，不要反问',
       'code': '只生成代码，不要提供多个方案',
       'brainstorming': '必须列举至少5个选项',
       'translation': '只翻译，不要解释'
   }
   ```

3. **添加负样本**
   ```python
   # 教模型什么是错误的
   negative_examples = [
       {
           'task': 'qa',
           'bad_output': '以下是3个答案：1... 2... 3...',
           'feedback': '错误：问答任务应该给出唯一答案'
       }
   ]
   ```

### 测试结果

任务串扰减轻，但没有完全解决。

**问答：** 82% → 86% ✅
**翻译：** 62% → 70% ✅
**代码：** 80% → 84% ✅

但偶尔还是会串扰（约10%案例）

---

## 🟢 Version 1.0 - 分阶段训练（Day 9-12）

### 关键创新：分阶段训练策略

```python
# 第1阶段：所有任务混合（1 epoch）
# 目的：学习通用能力

# 第2阶段：按任务分组训练（1 epoch）
for task in tasks:
    train_on_task_data(task)
# 目的：强化任务特异性

# 第3阶段：困难任务额外训练（0.5 epoch）
hard_tasks = ['logic_reasoning', 'translation']
train_on_hard_tasks(hard_tasks)
```

### 其他改进

1. **保留通用数据（30%）**
   ```python
   # 避免灾难性遗忘
   general_conversation_data = 400条
   task_data = 1300条
   total = 1700条
   ```

2. **调整学习率**
   ```
   通用训练: 2e-4
   任务特化: 1e-4 (更小，精细调整)
   ```

### 最终测试结果

```
平均准确率: 82% (之前70%)
任务串扰率: 3% (之前10%)
能力遗忘: 基本解决
```

**各任务表现：**
- 问答: 88% ✅✅
- 代码: 86% ✅✅
- 翻译: 78% ✅
- 推理: 75% ✅
- 其他: 80-85% ✅

✅ **基本可用！**

---

## 🟢 Version 1.1 - 细节优化（Day 13-15）

### 最后的优化

1. **统一输出格式**
   ```
   代码: 统一用```python包裹
   列表: 统一用1. 2. 3.
   强调: 统一用**粗体**
   ```

2. **添加后处理**
   ```python
   def post_process(output, task):
       if task == 'code' and '```' not in output:
           output = f'```python\n{output}\n```'
       return output
   ```

3. **持续数据清洗**
   - 删除了23条质量差的数据
   - 补充了15条高质量对比样本

### 最终指标

```
平均准确率: 85%
最低任务: 76% (逻辑推理)
最高任务: 91% (摘要)
差距: 15% (可接受)

任务串扰: <2%
格式一致性: 95%
用户满意度: ⭐⭐⭐⭐
```

✅ **项目完成！**

---

## 📊 完整统计

### 时间成本
```
Day 1-2: 初始训练 v0.1
Day 3-5: 任务平衡 v0.2
Day 6-8: 强化边界 v0.3
Day 9-12: 分阶段训练 v1.0
Day 13-15: 细节优化 v1.1

总计: 15天 ≈ 3周
```

### 数据演进
```
v0.1: 1200条(不均) → 性能差距大
v0.2: 1300条(平衡) → 串扰严重
v0.3: 1300条(强化边界) → 改善有限
v1.0: 1700条(+通用) → 基本可用
v1.1: 1685条(清洗后) → 效果优秀
```

---

## 💡 多任务SFT核心经验

### 1. 任务平衡是艺术，不是数学

❌ **错误想法：**
```
每个任务100条 = 平衡 ✅
```

✅ **正确理解：**
```
需要考虑：
- 数量平衡
- 质量平衡
- 难度平衡
- 重要性权重
```

### 2. 任务串扰是多任务的魔鬼

这是单任务SFT不会遇到的问题！

**表现：**
- 问答时列举多个答案（头脑风暴影响）
- 翻译时过度解释（观点分析影响）
- 代码时反问用户（对话影响）

**解决：**
- 强化任务边界
- 添加负样本
- 分阶段训练

### 3. 不要忘记通用能力

纯任务数据会让模型失去自然对话能力。

**保留30%通用对话数据！**

### 4. 测试要全面

不能只看平均准确率！

**必须测试：**
- 每个任务的单独性能
- 任务串扰情况
- 格式一致性
- 通用能力保留

### 5. 迭代周期更长

单任务: 1周
多任务: 2-3周

**原因：**
- 问题更复杂
- 数据量更大
- 测试更耗时

---

## 🆚 多任务 vs 单任务总结

### 多任务SFT更难的地方

1. **任务平衡**
   - 单任务：只需关注一个领域
   - 多任务：需要平衡10+个任务

2. **任务串扰**
   - 单任务：不存在这个问题
   - 多任务：核心挑战

3. **测试复杂度**
   - 单任务：测试一个维度
   - 多任务：测试10+个维度

4. **迭代周期**
   - 单任务：1周
   - 多任务：2-3周

### 多任务SFT的优势

1. **泛化能力强**
   - 可以处理各种类型的任务
   - 不局限于单一领域

2. **更practical**
   - 一个模型解决多个问题
   - 更符合实际应用场景

3. **知识迁移**
   - 任务间可以相互借鉴
   - 某些共享能力可以提升

---

## 🎯 如果重新开始

### Day 1-3: 小规模高质量起步
```
1. 每个任务先准备20条精品数据
2. 确保质量和多样性
3. 训练第一版，快速发现问题
```

### Day 4-7: 针对性补充
```
1. 测试发现哪些任务需要加强
2. 补充数据至100条/任务
3. 重点解决任务串扰问题
```

### Day 8-10: 分阶段训练
```
1. 实施分阶段训练策略
2. 添加通用数据
3. 全面测试
```

### Day 11-12: 细节打磨
```
1. 统一格式
2. 清洗数据
3. 最终优化
```

**预计时间：12天（比实际15天快3天）**

---

## 📚 相关文件

- `data/data_filtering_multitask.py` - 多任务数据筛选
- `training/multitask_problems_diagnosis.py` - 问题诊断
- `README.md` - 项目说明
- `QUICKSTART.md` - 快速开始

---

## ✉️ 最重要的教训

<div class="highlight">
<h3>🎯 多任务SFT ≠ 简单地混合数据</h3>

需要：
1. ✅ 精心的任务平衡策略
2. ✅ 明确的任务边界设计
3. ✅ 系统的任务串扰解决方案
4. ✅ 更长的迭代和调试周期
5. ✅ 全面的多维度测试

**但如果做好了，一个模型顶十个！**
</div>

