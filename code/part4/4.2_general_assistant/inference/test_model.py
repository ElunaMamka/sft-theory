"""
4.1 ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹ - æ¨¡å‹æµ‹è¯•è„šæœ¬
æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹æ•ˆæœ
"""

import sys
import os
import torch

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model.model_utils import (
    load_tokenizer,
    load_lora_model_for_inference,
    format_instruction,
)


class TCMInferenceEngine:
    """ä¸­åŒ»æ¨ç†å¼•æ“"""
    
    def __init__(self, base_model_path: str, lora_adapter_path: str):
        self.base_model_path = base_model_path
        self.lora_adapter_path = lora_adapter_path
        
        print("="*80)
        print("ğŸ¥ ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹ - æ¨¡å‹æµ‹è¯•")
        print("="*80)
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        self.tokenizer = load_tokenizer(base_model_path)
        self.model = load_lora_model_for_inference(
            base_model_path=base_model_path,
            lora_adapter_path=lora_adapter_path,
            load_in_4bit=True,
        )
        
        self.model.eval()
        print("\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‡†å¤‡å°±ç»ªï¼\n")
    
    def generate(self, 
                prompt: str,
                max_new_tokens: int = 1024,
                temperature: float = 0.7,
                top_p: float = 0.9,
                top_k: int = 50,
                repetition_penalty: float = 1.1):
        """ç”Ÿæˆå›å¤"""
        
        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt")
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                do_sample=True,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        
        # Decode
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–å›å¤éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥çš„promptï¼‰
        response = response[len(prompt):].strip()
        
        return response
    
    def test_single_case(self, 
                        instruction: str, 
                        patient_input: str,
                        expected_keywords: list = None):
        """æµ‹è¯•å•ä¸ªæ¡ˆä¾‹"""
        print("="*80)
        print("ğŸ“‹ æµ‹è¯•æ¡ˆä¾‹")
        print("="*80)
        
        print(f"\nã€æ‚£è€…è¾“å…¥ã€‘")
        print(patient_input)
        
        # æ ¼å¼åŒ–prompt
        prompt = format_instruction(instruction, patient_input)
        
        # ç”Ÿæˆå›å¤
        print(f"\nã€æ¨¡å‹å›å¤ã€‘")
        response = self.generate(prompt)
        print(response)
        
        # æ£€æŸ¥å…³é”®è¯
        if expected_keywords:
            print(f"\nã€å…³é”®è¯æ£€æŸ¥ã€‘")
            for keyword in expected_keywords:
                if keyword in response:
                    print(f"  âœ… {keyword}")
                else:
                    print(f"  âŒ {keyword} (æœªæåŠ)")
        
        print("\n" + "="*80)
        return response


def run_test_suite():
    """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
    
    # é…ç½®
    BASE_MODEL_PATH = "baichuan-inc/Baichuan2-7B-Base"
    LORA_ADAPTER_PATH = "../training/output/tcm_sft_lora"
    
    # åˆ›å»ºæ¨ç†å¼•æ“
    engine = TCMInferenceEngine(BASE_MODEL_PATH, LORA_ADAPTER_PATH)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "instruction": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸­åŒ»å¸ˆï¼Œè¯·æ ¹æ®æ‚£è€…çš„ç—‡çŠ¶è¿›è¡Œé—®è¯Šå’Œè¯Šæ–­ã€‚",
            "input": "åŒ»ç”Ÿæ‚¨å¥½ï¼Œæˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¾ˆç´¯ï¼Œæ‰‹è„šå†°å‡‰ï¼Œåƒé¥­ä¹Ÿæ²¡ä»€ä¹ˆèƒƒå£ã€‚èˆŒå¤´é¢œè‰²æ¯”è¾ƒæ·¡ã€‚",
            "expected_keywords": ["è¾¨è¯", "è„¾", "é˜³è™š", "å»ºè®®", "æ–¹è¯"]
        },
        {
            "instruction": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸­åŒ»å¸ˆï¼Œè¯·æ ¹æ®æ‚£è€…çš„ç—‡çŠ¶è¿›è¡Œé—®è¯Šå’Œè¯Šæ–­ã€‚",
            "input": "æˆ‘è¿™æ®µæ—¶é—´å£å¹²å£è‹¦ï¼Œè„¾æ°”å¾ˆå¤§å®¹æ˜“å‘ç«ï¼Œå¤§ä¾¿ä¹Ÿå¾ˆå¹²ã€‚èˆŒå¤´çº¢ï¼Œè‹”é»„ã€‚",
            "expected_keywords": ["è‚", "æ¹¿çƒ­", "ç«æ—º", "æ²»åˆ™", "æ¸…çƒ­"]
        },
        {
            "instruction": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸­åŒ»å¸ˆï¼Œè¯·æ ¹æ®æ‚£è€…çš„ç—‡çŠ¶è¿›è¡Œé—®è¯Šå’Œè¯Šæ–­ã€‚",
            "input": "æœ€è¿‘å¿ƒæ…Œå¿ƒæ‚¸ï¼Œæ™šä¸Šå¤±çœ ï¼Œæ‰‹å¿ƒè„šå¿ƒå‘çƒ­ï¼Œå¤œé‡Œä¼šå‡ºæ±—ã€‚",
            "expected_keywords": ["å¿ƒ", "é˜´è™š", "å®‰ç¥", "å…»é˜´", "æ»‹é˜´"]
        },
    ]
    
    # è¿è¡Œæµ‹è¯•
    print("\n" + "="*80)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•")
    print("="*80)
    
    results = []
    for idx, case in enumerate(test_cases, 1):
        print(f"\n\n{'='*80}")
        print(f"æµ‹è¯•ç”¨ä¾‹ {idx}/{len(test_cases)}")
        print(f"{'='*80}")
        
        response = engine.test_single_case(
            instruction=case["instruction"],
            patient_input=case["input"],
            expected_keywords=case.get("expected_keywords"),
        )
        
        results.append({
            "case_id": idx,
            "input": case["input"],
            "response": response,
        })
    
    # æµ‹è¯•æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*80)
    print(f"\næ€»æµ‹è¯•æ¡ˆä¾‹: {len(test_cases)}")
    print(f"å®Œæˆæµ‹è¯•: {len(results)}")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    import json
    with open('test_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: test_results.json")


def compare_with_base_model():
    """å¯¹æ¯”åŸºåº§æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹çš„æ•ˆæœ"""
    print("="*80)
    print("ğŸ”¬ åŸºåº§æ¨¡å‹ vs å¾®è°ƒæ¨¡å‹ å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    BASE_MODEL_PATH = "baichuan-inc/Baichuan2-7B-Base"
    LORA_ADAPTER_PATH = "../training/output/tcm_sft_lora"
    
    # æµ‹è¯•é—®é¢˜
    instruction = "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸­åŒ»å¸ˆï¼Œè¯·æ ¹æ®æ‚£è€…çš„ç—‡çŠ¶è¿›è¡Œé—®è¯Šå’Œè¯Šæ–­ã€‚"
    patient_input = "åŒ»ç”Ÿï¼Œæˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿè§‰å¾ˆç´¯ï¼Œæ‰‹è„šå†°å‡‰ï¼Œåƒé¥­æ²¡èƒƒå£ï¼ŒèˆŒå¤´é¢œè‰²æ·¡ã€‚"
    
    print(f"\næ‚£è€…é—®é¢˜: {patient_input}\n")
    
    # 1. åŸºåº§æ¨¡å‹
    print("-" * 80)
    print("ã€åŸºåº§æ¨¡å‹å›å¤ã€‘ï¼ˆæœªç»SFTå¾®è°ƒï¼‰")
    print("-" * 80)
    
    tokenizer = load_tokenizer(BASE_MODEL_PATH)
    base_model = load_lora_model_for_inference(
        base_model_path=BASE_MODEL_PATH,
        lora_adapter_path=None,  # ä¸åŠ è½½LoRA
        load_in_4bit=True,
    )
    
    prompt = format_instruction(instruction, patient_input)
    inputs = tokenizer(prompt, return_tensors="pt").to(base_model.device)
    
    with torch.no_grad():
        outputs = base_model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
        )
    
    base_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    base_response = base_response[len(prompt):].strip()
    print(base_response)
    
    # 2. å¾®è°ƒæ¨¡å‹
    print("\n" + "-" * 80)
    print("ã€å¾®è°ƒæ¨¡å‹å›å¤ã€‘ï¼ˆç»è¿‡SFTå¾®è°ƒï¼‰")
    print("-" * 80)
    
    engine = TCMInferenceEngine(BASE_MODEL_PATH, LORA_ADAPTER_PATH)
    sft_response = engine.generate(prompt, max_new_tokens=512)
    print(sft_response)
    
    # 3. å¯¹æ¯”åˆ†æ
    print("\n" + "="*80)
    print("ğŸ“Š æ•ˆæœå¯¹æ¯”åˆ†æ")
    print("="*80)
    
    analysis_criteria = {
        "ä¸“ä¸šæœ¯è¯­": ["è¯å‹", "è¾¨è¯", "æ²»åˆ™", "æ–¹è¯", "è„è…‘"],
        "ç»“æ„å®Œæ•´": ["ç—‡çŠ¶", "åˆ†æ", "å»ºè®®", "æ²»ç–—", "è°ƒç†"],
        "ä¸­åŒ»æ€ç»´": ["é˜´é˜³", "è™šå®", "å¯’çƒ­", "æ°”è¡€"],
    }
    
    for criterion, keywords in analysis_criteria.items():
        print(f"\nã€{criterion}ã€‘")
        base_count = sum(1 for kw in keywords if kw in base_response)
        sft_count = sum(1 for kw in keywords if kw in sft_response)
        
        print(f"  åŸºåº§æ¨¡å‹: {base_count}/{len(keywords)} ä¸ªå…³é”®è¯")
        print(f"  å¾®è°ƒæ¨¡å‹: {sft_count}/{len(keywords)} ä¸ªå…³é”®è¯")
        
        if sft_count > base_count:
            print(f"  âœ… å¾®è°ƒæ¨¡å‹æ›´å¥½")
        elif sft_count == base_count:
            print(f"  â– æŒå¹³")
        else:
            print(f"  âŒ åŸºåº§æ¨¡å‹æ›´å¥½")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æµ‹è¯•ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹æ¨¡å‹')
    parser.add_argument('--mode', type=str, default='test', 
                       choices=['test', 'compare'],
                       help='æµ‹è¯•æ¨¡å¼: test(æµ‹è¯•å¥—ä»¶) | compare(å¯¹æ¯”æµ‹è¯•)')
    
    args = parser.parse_args()
    
    if args.mode == 'test':
        run_test_suite()
    elif args.mode == 'compare':
        compare_with_base_model()


if __name__ == "__main__":
    main()
    
    print("\n" + "="*80)
    print("æç¤º:")
    print("  python test_model.py --mode test     # è¿è¡Œæµ‹è¯•å¥—ä»¶")
    print("  python test_model.py --mode compare  # å¯¹æ¯”åŸºåº§å’Œå¾®è°ƒæ¨¡å‹")
    print("="*80)

