"""
4.1 ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹ - æ¨¡å‹é…ç½®
å®šä¹‰æ¨¡å‹å‚æ•°ã€LoRAé…ç½®ç­‰
"""

from dataclasses import dataclass, field
from typing import Optional


@dataclass
class ModelConfig:
    """æ¨¡å‹åŸºç¡€é…ç½®"""
    
    # åŸºåº§æ¨¡å‹é€‰æ‹©
    model_name_or_path: str = "baichuan-inc/Baichuan2-7B-Base"
    # å…¶ä»–å¯é€‰æ¨¡å‹:
    # - "Qwen/Qwen-7B"
    # - "THUDM/chatglm3-6b"
    # - "01-ai/Yi-6B"
    
    # æ¨¡å‹å‚æ•°
    max_length: int = 2048  # æœ€å¤§åºåˆ—é•¿åº¦
    cache_dir: Optional[str] = "./model_cache"  # æ¨¡å‹ç¼“å­˜ç›®å½•
    
    # é‡åŒ–é…ç½®ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
    load_in_8bit: bool = False  # 8-bité‡åŒ–
    load_in_4bit: bool = True   # 4-bité‡åŒ–ï¼ˆæ¨èï¼‰
    
    # Tokenizeré…ç½®
    trust_remote_code: bool = True  # ä¿¡ä»»è¿œç¨‹ä»£ç 
    use_fast_tokenizer: bool = True


@dataclass
class LoRAConfig:
    """LoRAå¾®è°ƒé…ç½®"""
    
    # LoRAæ ¸å¿ƒå‚æ•°
    r: int = 8  # LoRA rankï¼ˆç§©ï¼‰
    # rçš„é€‰æ‹©:
    # - r=4: æè‡´å‚æ•°æ•ˆç‡ï¼Œé€‚åˆèµ„æºå—é™
    # - r=8: æ¨èå€¼ï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
    # - r=16: æ›´å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡
    # - r=64: æ¥è¿‘å…¨å‚æ•°å¾®è°ƒæ•ˆæœ
    
    lora_alpha: int = 16  # LoRAç¼©æ”¾å‚æ•°
    # ä¸€èˆ¬è®¾ç½®ä¸º r çš„ 2å€
    
    lora_dropout: float = 0.05  # Dropoutæ¦‚ç‡
    
    # ç›®æ ‡æ¨¡å—ï¼ˆå“ªäº›å±‚ä½¿ç”¨LoRAï¼‰
    target_modules: list = field(default_factory=lambda: [
        "q_proj",   # QueryæŠ•å½±
        "k_proj",   # KeyæŠ•å½±  
        "v_proj",   # ValueæŠ•å½±
        "o_proj",   # OutputæŠ•å½±
        "gate_proj", # MLP Gate
        "up_proj",   # MLP Up
        "down_proj", # MLP Down
    ])
    # æ³¨æ„ï¼šä¸åŒæ¨¡å‹çš„å±‚åç§°å¯èƒ½ä¸åŒ
    
    # ä»»åŠ¡ç±»å‹
    task_type: str = "CAUSAL_LM"  # å› æœè¯­è¨€æ¨¡å‹
    
    # åç½®å¤„ç†
    bias: str = "none"  # "none" | "all" | "lora_only"
    
    def get_trainable_params(self, base_params: int) -> dict:
        """è®¡ç®—å¯è®­ç»ƒå‚æ•°é‡"""
        # LoRAå‚æ•°é‡ = 2 * d * r * num_layers
        # å‡è®¾7Bæ¨¡å‹ï¼Œhidden_size=4096ï¼Œ32å±‚ï¼Œ7ä¸ªç›®æ ‡æ¨¡å—
        d = 4096
        num_layers = 32
        num_targets = len(self.target_modules)
        
        lora_params = 2 * d * self.r * num_layers * num_targets
        
        return {
            'base_params': base_params,
            'lora_params': lora_params,
            'trainable_ratio': lora_params / base_params,
            'trainable_percentage': f"{(lora_params / base_params * 100):.3f}%"
        }


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    
    # è¾“å‡ºç›®å½•
    output_dir: str = "./output/tcm_sft_lora"
    
    # è®­ç»ƒè½®æ•°
    num_train_epochs: int = 3
    
    # Batch size
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    # å®é™…batch_size = per_device_train_batch_size * gradient_accumulation_steps
    # = 4 * 4 = 16
    
    # å­¦ä¹ ç‡
    learning_rate: float = 2e-4  # LoRAæ¨èå­¦ä¹ ç‡
    # æ³¨æ„ï¼šLoRAçš„å­¦ä¹ ç‡é€šå¸¸æ¯”å…¨å‚æ•°å¾®è°ƒå¤§10å€
    # å…¨å‚æ•°: 1e-5 ~ 5e-5
    # LoRA: 1e-4 ~ 5e-4
    
    # ä¼˜åŒ–å™¨
    optim: str = "adamw_torch"
    # å¯é€‰: "adamw_8bit" (èŠ‚çœæ˜¾å­˜)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    lr_scheduler_type: str = "cosine"
    warmup_ratio: float = 0.03  # warmupæ­¥æ•°å æ¯”
    
    # æƒé‡è¡°å‡
    weight_decay: float = 0.01
    
    # æ¢¯åº¦è£å‰ª
    max_grad_norm: float = 1.0
    
    # ä¿å­˜ç­–ç•¥
    save_strategy: str = "steps"
    save_steps: int = 100
    save_total_limit: int = 3  # æœ€å¤šä¿å­˜3ä¸ªcheckpoint
    
    # è¯„ä¼°ç­–ç•¥
    evaluation_strategy: str = "steps"
    eval_steps: int = 100
    
    # æ—¥å¿—
    logging_steps: int = 10
    logging_dir: str = "./logs"
    report_to: str = "tensorboard"  # "wandb" | "tensorboard" | "none"
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    fp16: bool = False  # FP16
    bf16: bool = True   # BF16ï¼ˆæ¨èï¼Œæ›´ç¨³å®šï¼‰
    # æ³¨æ„ï¼šéœ€è¦GPUæ”¯æŒï¼ˆA100, H100ç­‰ï¼‰
    
    # å…¶ä»–
    seed: int = 42
    dataloader_num_workers: int = 4
    remove_unused_columns: bool = False
    
    # DeepSpeedé…ç½®ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰
    deepspeed: Optional[str] = None  # DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„


def print_config_summary():
    """æ‰“å°é…ç½®æ‘˜è¦"""
    print("="*80)
    print("âš™ï¸  æ¨¡å‹é…ç½®æ‘˜è¦")
    print("="*80)
    
    model_cfg = ModelConfig()
    lora_cfg = LoRAConfig()
    train_cfg = TrainingConfig()
    
    print(f"\nã€åŸºåº§æ¨¡å‹ã€‘")
    print(f"  æ¨¡å‹: {model_cfg.model_name_or_path}")
    print(f"  æœ€å¤§é•¿åº¦: {model_cfg.max_length}")
    print(f"  é‡åŒ–: {'4-bit' if model_cfg.load_in_4bit else '8-bit' if model_cfg.load_in_8bit else 'æ— '}")
    
    print(f"\nã€LoRAé…ç½®ã€‘")
    print(f"  Rank (r): {lora_cfg.r}")
    print(f"  Alpha: {lora_cfg.lora_alpha}")
    print(f"  Dropout: {lora_cfg.lora_dropout}")
    print(f"  ç›®æ ‡æ¨¡å—: {', '.join(lora_cfg.target_modules)}")
    
    # å‚æ•°é‡ä¼°ç®—
    base_params = 7_000_000_000  # 7Bæ¨¡å‹
    params_info = lora_cfg.get_trainable_params(base_params)
    print(f"\nã€å‚æ•°é‡ã€‘")
    print(f"  åŸºåº§å‚æ•°: {params_info['base_params']:,}")
    print(f"  LoRAå‚æ•°: {params_info['lora_params']:,}")
    print(f"  è®­ç»ƒæ¯”ä¾‹: {params_info['trainable_percentage']}")
    
    print(f"\nã€è®­ç»ƒé…ç½®ã€‘")
    print(f"  è®­ç»ƒè½®æ•°: {train_cfg.num_train_epochs}")
    print(f"  æœ‰æ•ˆBatch Size: {train_cfg.per_device_train_batch_size * train_cfg.gradient_accumulation_steps}")
    print(f"  å­¦ä¹ ç‡: {train_cfg.learning_rate}")
    print(f"  ä¼˜åŒ–å™¨: {train_cfg.optim}")
    print(f"  LRè°ƒåº¦: {train_cfg.lr_scheduler_type}")
    print(f"  æ··åˆç²¾åº¦: {'BF16' if train_cfg.bf16 else 'FP16' if train_cfg.fp16 else 'FP32'}")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    print_config_summary()
    
    # æ˜¾ç¤ºé…ç½®å»ºè®®
    print("\nğŸ’¡ é…ç½®å»ºè®®:")
    print("\n  ã€æ˜¾å­˜å ç”¨ä¼°ç®—ã€‘")
    print("    - 4-bité‡åŒ– + LoRA r=8: ~8GB")
    print("    - 8-bité‡åŒ– + LoRA r=8: ~12GB")
    print("    - FP16 + LoRA r=8: ~20GB")
    print("    - å…¨å‚æ•°å¾®è°ƒ: ~56GB+")
    
    print("\n  ã€LoRA ranké€‰æ‹©ã€‘")
    print("    - r=4:  æè‡´æ•ˆç‡ï¼Œé€‚åˆç®€å•ä»»åŠ¡")
    print("    - r=8:  æ¨èå€¼ï¼Œæ€§èƒ½å’Œæ•ˆç‡å¹³è¡¡ âœ…")
    print("    - r=16: æ›´å¼ºèƒ½åŠ›ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡")
    print("    - r=64: æ¥è¿‘å…¨å‚æ•°æ•ˆæœ")
    
    print("\n  ã€å­¦ä¹ ç‡è®¾ç½®ã€‘")
    print("    - LoRA: 1e-4 ~ 5e-4ï¼ˆæ¨è2e-4ï¼‰")
    print("    - å…¨å‚æ•°: 1e-5 ~ 5e-5")
    print("    - æ³¨æ„ï¼šLoRAå­¦ä¹ ç‡é€šå¸¸æ˜¯å…¨å‚æ•°çš„10å€")

