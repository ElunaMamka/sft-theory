"""
4.1 ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹ - æ¨¡å‹å·¥å…·å‡½æ•°
åŠ è½½æ¨¡å‹ã€tokenizerã€LoRAç­‰é€šç”¨å‡½æ•°
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    PeftModel,
)
from typing import Optional


def load_tokenizer(model_name_or_path: str, 
                   trust_remote_code: bool = True,
                   use_fast: bool = True):
    """
    åŠ è½½tokenizer
    """
    print(f"ğŸ“¥ åŠ è½½Tokenizer: {model_name_or_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path,
        trust_remote_code=trust_remote_code,
        use_fast=use_fast,
    )
    
    # è®¾ç½®padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print("  âš ï¸ pad_tokenæœªè®¾ç½®ï¼Œä½¿ç”¨eos_tokenä½œä¸ºpad_token")
    
    # è®¾ç½®paddingæ–¹å‘ï¼ˆå·¦paddingç”¨äºç”Ÿæˆä»»åŠ¡ï¼‰
    tokenizer.padding_side = 'left'
    
    print(f"  âœ… TokenizeråŠ è½½å®Œæˆ")
    print(f"     è¯è¡¨å¤§å°: {len(tokenizer)}")
    print(f"     pad_token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})")
    print(f"     eos_token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})")
    
    return tokenizer


def load_base_model(model_name_or_path: str,
                    load_in_4bit: bool = True,
                    load_in_8bit: bool = False,
                    device_map: str = "auto",
                    trust_remote_code: bool = True):
    """
    åŠ è½½åŸºåº§æ¨¡å‹ï¼ˆæ”¯æŒé‡åŒ–ï¼‰
    """
    print(f"\nğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {model_name_or_path}")
    
    # é‡åŒ–é…ç½®
    quantization_config = None
    if load_in_4bit:
        print("  ğŸ”§ å¯ç”¨4-bité‡åŒ–ï¼ˆQLoRAï¼‰")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
    elif load_in_8bit:
        print("  ğŸ”§ å¯ç”¨8-bité‡åŒ–")
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
        )
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        quantization_config=quantization_config,
        device_map=device_map,
        trust_remote_code=trust_remote_code,
        torch_dtype=torch.bfloat16 if not (load_in_4bit or load_in_8bit) else None,
    )
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"  âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"     æ€»å‚æ•°: {total_params:,}")
    print(f"     å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"     è®¾å¤‡æ˜ å°„: {device_map}")
    
    # æ˜¾å­˜å ç”¨
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**3
        memory_reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"     GPUæ˜¾å­˜: å·²åˆ†é… {memory_allocated:.2f}GB / å·²ä¿ç•™ {memory_reserved:.2f}GB")
    
    return model


def prepare_model_for_lora(model, use_gradient_checkpointing: bool = True):
    """
    ä¸ºLoRAè®­ç»ƒå‡†å¤‡æ¨¡å‹
    """
    print(f"\nğŸ”§ å‡†å¤‡æ¨¡å‹ç”¨äºLoRAè®­ç»ƒ...")
    
    # å‡†å¤‡é‡åŒ–æ¨¡å‹
    model = prepare_model_for_kbit_training(
        model,
        use_gradient_checkpointing=use_gradient_checkpointing
    )
    
    if use_gradient_checkpointing:
        model.config.use_cache = False
        print("  âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆé™ä½æ˜¾å­˜å ç”¨ï¼‰")
    
    return model


def add_lora_adapters(model, 
                      r: int = 8,
                      lora_alpha: int = 16,
                      lora_dropout: float = 0.05,
                      target_modules: Optional[list] = None):
    """
    æ·»åŠ LoRAé€‚é…å™¨
    """
    print(f"\nğŸ”§ æ·»åŠ LoRAé€‚é…å™¨...")
    
    if target_modules is None:
        # é»˜è®¤ç›®æ ‡æ¨¡å—ï¼ˆé€‚ç”¨äºå¤§å¤šæ•°Transformeræ¨¡å‹ï¼‰
        target_modules = [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ]
    
    # LoRAé…ç½®
    lora_config = LoraConfig(
        r=r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules,
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    
    # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
    model.print_trainable_parameters()
    
    return model


def load_model_for_training(model_name_or_path: str,
                           load_in_4bit: bool = True,
                           lora_r: int = 8,
                           lora_alpha: int = 16,
                           lora_dropout: float = 0.05):
    """
    ä¸€é”®åŠ è½½å¹¶é…ç½®è®­ç»ƒæ¨¡å‹ï¼ˆå®Œæ•´æµç¨‹ï¼‰
    """
    print("="*80)
    print("ğŸš€ åŠ è½½å¹¶é…ç½®è®­ç»ƒæ¨¡å‹")
    print("="*80)
    
    # 1. åŠ è½½åŸºåº§æ¨¡å‹
    model = load_base_model(
        model_name_or_path=model_name_or_path,
        load_in_4bit=load_in_4bit,
    )
    
    # 2. å‡†å¤‡LoRAè®­ç»ƒ
    model = prepare_model_for_lora(model)
    
    # 3. æ·»åŠ LoRAé€‚é…å™¨
    model = add_lora_adapters(
        model,
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
    )
    
    print("\n" + "="*80)
    print("âœ… æ¨¡å‹å‡†å¤‡å®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼")
    print("="*80)
    
    return model


def load_lora_model_for_inference(base_model_path: str,
                                  lora_adapter_path: str,
                                  load_in_4bit: bool = False):
    """
    åŠ è½½LoRAæ¨¡å‹ç”¨äºæ¨ç†
    """
    print("="*80)
    print("ğŸš€ åŠ è½½LoRAæ¨¡å‹ç”¨äºæ¨ç†")
    print("="*80)
    
    # 1. åŠ è½½åŸºåº§æ¨¡å‹
    base_model = load_base_model(
        model_name_or_path=base_model_path,
        load_in_4bit=load_in_4bit,
    )
    
    # 2. åŠ è½½LoRAæƒé‡
    print(f"\nğŸ“¥ åŠ è½½LoRAé€‚é…å™¨: {lora_adapter_path}")
    model = PeftModel.from_pretrained(
        base_model,
        lora_adapter_path,
    )
    
    # 3. åˆå¹¶æƒé‡ï¼ˆå¯é€‰ï¼Œç”¨äºåŠ é€Ÿæ¨ç†ï¼‰
    # model = model.merge_and_unload()
    # print("  âœ… LoRAæƒé‡å·²åˆå¹¶åˆ°åŸºåº§æ¨¡å‹")
    
    print("  âœ… LoRAæ¨¡å‹åŠ è½½å®Œæˆ")
    
    return model


def format_instruction(instruction: str, 
                      input_text: str,
                      output_text: Optional[str] = None) -> str:
    """
    æ ¼å¼åŒ–æŒ‡ä»¤ä¸ºè®­ç»ƒæ ¼å¼
    é‡‡ç”¨Llama-2é£æ ¼çš„prompt
    """
    if output_text is None:
        # æ¨ç†æ¨¡å¼
        prompt = f"""<s>[INST] <<SYS>>
{instruction}
<</SYS>>

{input_text} [/INST]"""
    else:
        # è®­ç»ƒæ¨¡å¼
        prompt = f"""<s>[INST] <<SYS>>
{instruction}
<</SYS>>

{input_text} [/INST] {output_text} </s>"""
    
    return prompt


if __name__ == "__main__":
    # æµ‹è¯•åŠ è½½æµç¨‹
    print("æµ‹è¯•æ¨¡å‹åŠ è½½æµç¨‹...\n")
    
    # æ³¨æ„ï¼šè¿™åªæ˜¯æ¼”ç¤ºï¼Œå®é™…è¿è¡Œéœ€è¦å…ˆä¸‹è½½æ¨¡å‹
    model_name = "baichuan-inc/Baichuan2-7B-Base"
    
    print("ç¤ºä¾‹é…ç½®:")
    print(f"  åŸºåº§æ¨¡å‹: {model_name}")
    print(f"  é‡åŒ–: 4-bit")
    print(f"  LoRA rank: 8")
    print(f"  LoRA alpha: 16")
    
    print("\nå¦‚éœ€å®é™…åŠ è½½ï¼Œè¯·å–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä»£ç :")
    print("# model = load_model_for_training(model_name)")

