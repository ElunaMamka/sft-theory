# 4.1 中医问诊助手 - 训练配置文件
# 可以通过修改此文件来调整训练参数

# ==================== 模型配置 ====================
model:
  name_or_path: "baichuan-inc/Baichuan2-7B-Base"
  # 其他可选模型:
  # - "Qwen/Qwen-7B"
  # - "THUDM/chatglm3-6b"  
  # - "01-ai/Yi-6B"
  
  max_length: 2048
  cache_dir: "./model_cache"
  
  # 量化设置（节省显存）
  load_in_4bit: true   # 推荐：4-bit量化
  load_in_8bit: false
  
  trust_remote_code: true
  use_fast_tokenizer: true

# ==================== LoRA配置 ====================
lora:
  r: 8  # LoRA rank
  # r的选择建议:
  # - 4: 极致效率
  # - 8: 推荐值 ✅
  # - 16: 更强能力
  # - 64: 接近全参数
  
  lora_alpha: 16  # 通常是 r 的 2倍
  lora_dropout: 0.05
  
  # 目标模块（根据不同模型可能需要调整）
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  
  bias: "none"
  task_type: "CAUSAL_LM"

# ==================== 训练配置 ====================
training:
  # 输出目录
  output_dir: "./output/tcm_sft_lora"
  
  # 训练轮数
  num_train_epochs: 3
  
  # Batch size设置
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  # 实际 batch_size = 4 * 4 = 16
  
  # 学习率
  learning_rate: 2e-4  # LoRA推荐: 1e-4 ~ 5e-4
  lr_scheduler_type: "cosine"  # cosine | linear | constant
  warmup_ratio: 0.03  # warmup步数占比
  
  # 优化器
  optim: "adamw_torch"  # adamw_torch | adamw_8bit
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # 混合精度
  fp16: false
  bf16: true  # 推荐（需要支持的GPU）
  
  # 保存策略
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3
  
  # 评估策略
  evaluation_strategy: "steps"
  eval_steps: 100
  
  # 日志
  logging_steps: 10
  logging_dir: "./logs"
  report_to: "tensorboard"  # tensorboard | wandb | none
  
  # 其他
  seed: 42
  dataloader_num_workers: 4
  remove_unused_columns: false
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"

# ==================== 数据配置 ====================
data:
  train_data_path: "../data/processed/train"
  eval_data_path: "../data/processed/val"
  
  # 数据增强（可选）
  augmentation:
    enabled: false
    techniques:
      - "paraphrase"
      - "back_translation"

# ==================== 分布式训练配置 ====================
distributed:
  # 是否启用分布式
  enabled: false
  
  # 分布式后端
  backend: "nccl"  # nccl | gloo
  
  # DeepSpeed配置
  deepspeed:
    enabled: false
    config_file: "deepspeed_config.json"
    zero_stage: 2  # 0 | 1 | 2 | 3

# ==================== 推理配置 ====================
inference:
  max_new_tokens: 1024
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  do_sample: true

# ==================== 硬件配置参考 ====================
# 显存占用估算:
#   单GPU训练:
#     - 4-bit + LoRA r=8: ~8GB
#     - 8-bit + LoRA r=8: ~12GB
#     - FP16 + LoRA r=8: ~20GB
#
#   4x GPU训练:
#     - 可以使用更大的batch size
#     - 或训练更大的模型
#
# 训练时间估算（单GPU V100）:
#   - 1000条数据，3 epochs
#   - batch_size=16, 约2-3小时
#
# 性能优化建议:
#   1. 使用bf16混合精度（快20%）
#   2. 增大batch size（充分利用GPU）
#   3. 使用gradient checkpointing（节省显存）
#   4. 多GPU并行训练（接近线性加速）

