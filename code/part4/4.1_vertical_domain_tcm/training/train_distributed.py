"""
4.1 ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹ - åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬
æ”¯æŒå¤šGPUè®­ç»ƒã€DeepSpeedã€FSDPç­‰
"""

import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from accelerate import Accelerator
from train_lora import TCMTrainer
from model.model_config import ModelConfig, LoRAConfig, TrainingConfig


def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    else:
        print("æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼ç¯å¢ƒï¼Œä½¿ç”¨å•GPUè®­ç»ƒ")
        return False
    
    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend='nccl')
    
    if rank == 0:
        print(f"åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®:")
        print(f"  World size: {world_size}")
        print(f"  Rank: {rank}")
        print(f"  Local rank: {local_rank}")
    
    return True


def train_with_accelerate():
    """
    ä½¿ç”¨Accelerateè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
    Accelerateä¼šè‡ªåŠ¨å¤„ç†å¤šGPUã€æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ç­‰
    """
    print("="*80)
    print("ğŸš€ ä½¿ç”¨Accelerateè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
    print("="*80)
    
    # åˆ›å»ºAccelerator
    accelerator = Accelerator(
        gradient_accumulation_steps=4,
        mixed_precision='bf16',  # 'fp16' | 'bf16' | 'no'
        log_with='tensorboard',
        project_dir='./logs',
    )
    
    if accelerator.is_main_process:
        print(f"\nåŠ é€Ÿå™¨ä¿¡æ¯:")
        print(f"  è®¾å¤‡: {accelerator.device}")
        print(f"  åˆ†å¸ƒå¼ç±»å‹: {accelerator.distributed_type}")
        print(f"  è¿›ç¨‹æ•°: {accelerator.num_processes}")
        print(f"  æ··åˆç²¾åº¦: {accelerator.mixed_precision}")
    
    # é…ç½®
    model_cfg = ModelConfig(
        model_name_or_path="baichuan-inc/Baichuan2-7B-Base",
        load_in_4bit=False,  # åˆ†å¸ƒå¼è®­ç»ƒæ—¶ä¸ä½¿ç”¨é‡åŒ–
    )
    
    lora_cfg = LoRAConfig(r=8, lora_alpha=16)
    
    train_cfg = TrainingConfig(
        output_dir="./output/tcm_sft_lora_distributed",
        num_train_epochs=3,
        per_device_train_batch_size=2,  # æ¯ä¸ªGPUçš„batch size
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = TCMTrainer(model_cfg, lora_cfg, train_cfg)
    trainer.setup()
    trainer.load_data(
        train_data_path="../data/processed/train",
        eval_data_path="../data/processed/val",
    )
    
    # ä½¿ç”¨Accelerateçš„Trainerä¼šè‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼
    trainer.train()
    
    if accelerator.is_main_process:
        print("\nâœ… åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆï¼")


def train_with_deepspeed():
    """
    ä½¿ç”¨DeepSpeedè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
    DeepSpeedæä¾›ZeROä¼˜åŒ–ï¼Œå¯ä»¥è®­ç»ƒæ›´å¤§çš„æ¨¡å‹
    """
    print("="*80)
    print("ğŸš€ ä½¿ç”¨DeepSpeedè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
    print("="*80)
    
    # DeepSpeedé…ç½®
    deepspeed_config = {
        "train_batch_size": "auto",
        "train_micro_batch_size_per_gpu": "auto",
        "gradient_accumulation_steps": "auto",
        
        "optimizer": {
            "type": "AdamW",
            "params": {
                "lr": "auto",
                "betas": "auto",
                "eps": "auto",
                "weight_decay": "auto"
            }
        },
        
        "scheduler": {
            "type": "WarmupDecayLR",
            "params": {
                "warmup_min_lr": "auto",
                "warmup_max_lr": "auto",
                "warmup_num_steps": "auto",
                "total_num_steps": "auto"
            }
        },
        
        "fp16": {
            "enabled": False
        },
        
        "bf16": {
            "enabled": True
        },
        
        "zero_optimization": {
            "stage": 2,  # ZeRO stage 0/1/2/3
            "offload_optimizer": {
                "device": "cpu",  # å°†optimizerçŠ¶æ€offloadåˆ°CPU
                "pin_memory": True
            },
            "allgather_partitions": True,
            "allgather_bucket_size": 2e8,
            "reduce_scatter": True,
            "reduce_bucket_size": 2e8,
            "overlap_comm": True,
            "contiguous_gradients": True
        },
        
        "gradient_clipping": 1.0,
        
        "steps_per_print": 10,
        "wall_clock_breakdown": False
    }
    
    # ä¿å­˜DeepSpeedé…ç½®
    import json
    with open('deepspeed_config.json', 'w') as f:
        json.dump(deepspeed_config, f, indent=2)
    
    print("DeepSpeedé…ç½®å·²ä¿å­˜åˆ°: deepspeed_config.json")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("  deepspeed --num_gpus=4 train_distributed.py")
    
    # é…ç½®
    model_cfg = ModelConfig(
        model_name_or_path="baichuan-inc/Baichuan2-7B-Base",
        load_in_4bit=False,
    )
    
    lora_cfg = LoRAConfig(r=8, lora_alpha=16)
    
    train_cfg = TrainingConfig(
        output_dir="./output/tcm_sft_lora_deepspeed",
        num_train_epochs=3,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        deepspeed='deepspeed_config.json',
    )
    
    # è®­ç»ƒ
    trainer = TCMTrainer(model_cfg, lora_cfg, train_cfg)
    trainer.setup()
    trainer.load_data(
        train_data_path="../data/processed/train",
        eval_data_path="../data/processed/val",
    )
    trainer.train()


def print_usage_guide():
    """æ‰“å°ä½¿ç”¨æŒ‡å—"""
    print("="*80)
    print("ğŸ“– åˆ†å¸ƒå¼è®­ç»ƒä½¿ç”¨æŒ‡å—")
    print("="*80)
    
    print("\nã€æ–¹å¼1ï¼štorchrunï¼ˆæ¨èï¼‰ã€‘")
    print("  å•æœºå¤šå¡:")
    print("    torchrun --nproc_per_node=4 train_distributed.py")
    print()
    print("  å¤šæœºå¤šå¡:")
    print("    # ä¸»èŠ‚ç‚¹")
    print("    torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 \\")
    print("             --master_addr=192.168.1.1 --master_port=29500 \\")
    print("             train_distributed.py")
    print("    # ä»èŠ‚ç‚¹")
    print("    torchrun --nproc_per_node=4 --nnodes=2 --node_rank=1 \\")
    print("             --master_addr=192.168.1.1 --master_port=29500 \\")
    print("             train_distributed.py")
    
    print("\nã€æ–¹å¼2ï¼šAccelerateã€‘")
    print("  1. é…ç½®:")
    print("     accelerate config")
    print("  2. è®­ç»ƒ:")
    print("     accelerate launch train_distributed.py")
    
    print("\nã€æ–¹å¼3ï¼šDeepSpeedã€‘")
    print("  å•æœºå¤šå¡:")
    print("    deepspeed --num_gpus=4 train_distributed.py")
    print()
    print("  å¤šæœºå¤šå¡:")
    print("    deepspeed --num_gpus=4 --num_nodes=2 \\")
    print("              --master_addr=192.168.1.1 --master_port=29500 \\")
    print("              train_distributed.py")
    
    print("\nã€æ€§èƒ½å¯¹æ¯”ã€‘")
    print("  å•GPU (V100 32GB):")
    print("    - Batch size: 4")
    print("    - è®­ç»ƒé€Ÿåº¦: ~2 samples/s")
    print("    - é¢„è®¡æ—¶é—´: 3å°æ—¶ï¼ˆ1000æ¡æ•°æ®ï¼Œ3 epochsï¼‰")
    print()
    print("  4x GPU (V100 32GB):")
    print("    - Batch size: 16 (4x4)")
    print("    - è®­ç»ƒé€Ÿåº¦: ~7 samples/s")
    print("    - é¢„è®¡æ—¶é—´: 45åˆ†é’Ÿ")
    print("    - åŠ é€Ÿæ¯”: çº¦4å€")
    
    print("\nã€æ˜¾å­˜ä¼˜åŒ–æŠ€å·§ã€‘")
    print("  1. æ¢¯åº¦æ£€æŸ¥ç‚¹: é™ä½40%æ˜¾å­˜ï¼Œå¢åŠ 20%æ—¶é—´")
    print("  2. ZeRO Stage 1: é™ä½4å€optimizeræ˜¾å­˜")
    print("  3. ZeRO Stage 2: é™ä½8å€optimizer+gradientæ˜¾å­˜")
    print("  4. ZeRO Stage 3: é™ä½Nå€æ‰€æœ‰å‚æ•°æ˜¾å­˜ï¼ˆN=GPUæ•°ï¼‰")
    print("  5. Offload to CPU: å‡ ä¹æ— é™æ˜¾å­˜ï¼Œä½†æ…¢50%")
    
    print("\n" + "="*80)


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        if mode == 'accelerate':
            train_with_accelerate()
        elif mode == 'deepspeed':
            train_with_deepspeed()
        else:
            print(f"æœªçŸ¥æ¨¡å¼: {mode}")
            print_usage_guide()
    else:
        print_usage_guide()
        print("\nç¤ºä¾‹:")
        print("  python train_distributed.py accelerate")
        print("  python train_distributed.py deepspeed")


if __name__ == "__main__":
    main()

