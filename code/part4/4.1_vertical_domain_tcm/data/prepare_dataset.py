"""
4.1 ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹ - æ•°æ®é¢„å¤„ç†å’Œæ ¼å¼åŒ–
å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„æ ¼å¼
"""

import json
import os
from typing import List, Dict
from datasets import Dataset
import pandas as pd


class TCMDatasetPreparer:
    """ä¸­åŒ»æ•°æ®é›†å‡†å¤‡å™¨"""
    
    def __init__(self, raw_data_path: str = 'tcm_raw_data.json'):
        self.raw_data_path = raw_data_path
        self.processed_data = []
        
    def load_raw_data(self) -> List[Dict]:
        """åŠ è½½åŸå§‹æ•°æ®"""
        with open(self.raw_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… åŠ è½½äº† {len(data)} æ¡åŸå§‹æ•°æ®")
        return data
    
    def format_for_training(self, data: List[Dict]) -> List[Dict]:
        """
        æ ¼å¼åŒ–æ•°æ®ä¸ºè®­ç»ƒæ ¼å¼
        é‡‡ç”¨ Alpaca æ ¼å¼
        """
        formatted_data = []
        
        for item in data:
            # Alpaca format with Chinese instruction
            formatted_item = {
                'instruction': item['instruction'],
                'input': item['input'],
                'output': item['output'],
                # ä¸ºäº†é€‚é…ä¸åŒçš„è®­ç»ƒæ¡†æ¶ï¼Œä¹Ÿæä¾›å®Œæ•´çš„ text å­—æ®µ
                'text': self._create_full_prompt(item)
            }
            formatted_data.append(formatted_item)
        
        print(f"âœ… æ ¼å¼åŒ–äº† {len(formatted_data)} æ¡æ•°æ®")
        return formatted_data
    
    def _create_full_prompt(self, item: Dict) -> str:
        """
        åˆ›å»ºå®Œæ•´çš„è®­ç»ƒæ–‡æœ¬
        æ ¼å¼ï¼š<s>[INST] <<SYS>>\n{instruction}\n<</SYS>>\n\n{input}[/INST]{output}</s>
        è¿™æ˜¯ Llama-2 çš„æŒ‡ä»¤æ ¼å¼
        """
        instruction = item['instruction']
        input_text = item['input']
        output = item['output']
        
        # Llama-2 style prompt
        prompt = f"""<s>[INST] <<SYS>>
{instruction}
<</SYS>>

{input_text} [/INST] {output} </s>"""
        
        return prompt
    
    def split_dataset(self, data: List[Dict], 
                     train_ratio: float = 0.9) -> tuple:
        """
        åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        """
        import random
        random.seed(42)
        random.shuffle(data)
        
        split_idx = int(len(data) * train_ratio)
        train_data = data[:split_idx]
        val_data = data[split_idx:]
        
        print(f"ğŸ“Š æ•°æ®åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {len(train_data)} æ¡")
        print(f"  éªŒè¯é›†: {len(val_data)} æ¡")
        
        return train_data, val_data
    
    def save_to_json(self, data: List[Dict], output_path: str):
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ä¿å­˜åˆ°: {output_path}")
    
    def save_to_huggingface_format(self, train_data: List[Dict], 
                                   val_data: List[Dict], 
                                   output_dir: str = 'processed'):
        """
        ä¿å­˜ä¸º HuggingFace datasets æ ¼å¼
        ä¾¿äºä½¿ç”¨ transformers çš„ Trainer
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»º Dataset å¯¹è±¡
        train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))
        val_dataset = Dataset.from_pandas(pd.DataFrame(val_data))
        
        # ä¿å­˜
        train_dataset.save_to_disk(f"{output_dir}/train")
        val_dataset.save_to_disk(f"{output_dir}/val")
        
        print(f"âœ… HuggingFaceæ ¼å¼æ•°æ®ä¿å­˜åˆ°: {output_dir}/")
    
    def analyze_dataset(self, data: List[Dict]):
        """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸ“Š æ•°æ®é›†åˆ†æ")
        print("="*80)
        
        # åŸºæœ¬ç»Ÿè®¡
        total_samples = len(data)
        print(f"\næ€»æ ·æœ¬æ•°: {total_samples}")
        
        # é•¿åº¦ç»Ÿè®¡
        input_lengths = [len(item['input']) for item in data]
        output_lengths = [len(item['output']) for item in data]
        
        print(f"\nè¾“å…¥é•¿åº¦ç»Ÿè®¡:")
        print(f"  æœ€å°å€¼: {min(input_lengths)}")
        print(f"  æœ€å¤§å€¼: {max(input_lengths)}")
        print(f"  å¹³å‡å€¼: {sum(input_lengths)/len(input_lengths):.0f}")
        
        print(f"\nè¾“å‡ºé•¿åº¦ç»Ÿè®¡:")
        print(f"  æœ€å°å€¼: {min(output_lengths)}")
        print(f"  æœ€å¤§å€¼: {max(output_lengths)}")
        print(f"  å¹³å‡å€¼: {sum(output_lengths)/len(output_lengths):.0f}")
        
        # Tokenä¼°ç®—ï¼ˆä¸­æ–‡å¹³å‡1.5å­—ç¬¦=1 tokenï¼‰
        avg_total_tokens = (sum(input_lengths) + sum(output_lengths)) / len(data) / 1.5
        print(f"\nå¹³å‡tokenæ•°ï¼ˆä¼°ç®—ï¼‰: {avg_total_tokens:.0f}")
        
        # å†…å®¹åˆ†æ
        print(f"\nå†…å®¹ç‰¹ç‚¹:")
        symptoms_keywords = ['ç—‡çŠ¶', 'ç–¼ç—›', 'ä¸é€‚', 'æ„Ÿè§‰']
        diagnosis_keywords = ['è¾¨è¯', 'è¯å‹', 'é˜´è™š', 'é˜³è™š', 'æ°”è™š', 'è¡€è™š']
        treatment_keywords = ['æ²»ç–—', 'æ–¹è¯', 'å»ºè®®', 'è°ƒç†']
        
        samples_with_symptoms = sum(1 for item in data 
                                   if any(kw in item['input'] for kw in symptoms_keywords))
        samples_with_diagnosis = sum(1 for item in data 
                                    if any(kw in item['output'] for kw in diagnosis_keywords))
        samples_with_treatment = sum(1 for item in data 
                                    if any(kw in item['output'] for kw in treatment_keywords))
        
        print(f"  åŒ…å«ç—‡çŠ¶æè¿°: {samples_with_symptoms}/{total_samples}")
        print(f"  åŒ…å«è¾¨è¯åˆ†æ: {samples_with_diagnosis}/{total_samples}")
        print(f"  åŒ…å«æ²»ç–—æ–¹æ¡ˆ: {samples_with_treatment}/{total_samples}")
        
        print("\n" + "="*80)
    
    def prepare_full_pipeline(self):
        """å®Œæ•´çš„æ•°æ®å‡†å¤‡æµç¨‹"""
        print("="*80)
        print("ğŸš€ å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹")
        print("="*80)
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        raw_data = self.load_raw_data()
        
        # 2. æ•°æ®åˆ†æ
        self.analyze_dataset(raw_data)
        
        # 3. æ ¼å¼åŒ–æ•°æ®
        formatted_data = self.format_for_training(raw_data)
        
        # 4. åˆ’åˆ†æ•°æ®é›†
        train_data, val_data = self.split_dataset(formatted_data)
        
        # 5. ä¿å­˜ä¸ºå¤šç§æ ¼å¼
        print("\nğŸ’¾ ä¿å­˜æ•°æ®...")
        
        # JSONæ ¼å¼
        self.save_to_json(train_data, 'train_data.json')
        self.save_to_json(val_data, 'val_data.json')
        
        # HuggingFaceæ ¼å¼
        self.save_to_huggingface_format(train_data, val_data)
        
        print("\n" + "="*80)
        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
        print("="*80)
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  - train_data.json (JSONæ ¼å¼è®­ç»ƒæ•°æ®)")
        print("  - val_data.json (JSONæ ¼å¼éªŒè¯æ•°æ®)")
        print("  - processed/train/ (HuggingFaceæ ¼å¼è®­ç»ƒæ•°æ®)")
        print("  - processed/val/ (HuggingFaceæ ¼å¼éªŒè¯æ•°æ®)")
        
        return train_data, val_data


def main():
    """ä¸»å‡½æ•°"""
    preparer = TCMDatasetPreparer('tcm_raw_data.json')
    train_data, val_data = preparer.prepare_full_pipeline()
    
    # æ‰“å°ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ç¤ºä¾‹
    print("\n" + "="*80)
    print("ğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹ï¼ˆå‰500å­—ç¬¦ï¼‰")
    print("="*80)
    print(train_data[0]['text'][:500])
    print("...")
    print("="*80)


if __name__ == "__main__":
    main()

