"""
4.1 ä¸­åŒ»é—®è¯ŠåŠ©æ‰‹ - æ•°æ®é›†æ·±åº¦åˆ†æ
åˆ†ææ•°æ®è´¨é‡ã€åˆ†å¸ƒç‰¹å¾ã€æ½œåœ¨é—®é¢˜
"""

import json
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter
import jieba
import re


class TCMDatasetAnalyzer:
    """ä¸­åŒ»æ•°æ®é›†åˆ†æå™¨"""
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.data = self.load_data()
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    
    def analyze_length_distribution(self):
        """åˆ†æé•¿åº¦åˆ†å¸ƒ"""
        print("\n" + "="*80)
        print("ğŸ“ é•¿åº¦åˆ†å¸ƒåˆ†æ")
        print("="*80)
        
        input_lengths = [len(item['input']) for item in self.data]
        output_lengths = [len(item['output']) for item in self.data]
        total_lengths = [len(item['input']) + len(item['output']) 
                        for item in self.data]
        
        # ç»Ÿè®¡ä¿¡æ¯
        def print_stats(name, lengths):
            print(f"\n{name}:")
            print(f"  æœ€å°: {min(lengths):,} å­—ç¬¦")
            print(f"  æœ€å¤§: {max(lengths):,} å­—ç¬¦")
            print(f"  å¹³å‡: {np.mean(lengths):,.0f} å­—ç¬¦")
            print(f"  ä¸­ä½æ•°: {np.median(lengths):,.0f} å­—ç¬¦")
            print(f"  æ ‡å‡†å·®: {np.std(lengths):,.0f} å­—ç¬¦")
        
        print_stats("è¾“å…¥é•¿åº¦", input_lengths)
        print_stats("è¾“å‡ºé•¿åº¦", output_lengths)
        print_stats("æ€»é•¿åº¦", total_lengths)
        
        # Tokenä¼°ç®—
        avg_tokens = np.mean(total_lengths) / 1.5  # ä¸­æ–‡çº¦1.5å­—ç¬¦=1token
        print(f"\né¢„ä¼°å¹³å‡tokenæ•°: {avg_tokens:.0f}")
        print(f"é¢„ä¼°æœ€å¤§tokenæ•°: {max(total_lengths)/1.5:.0f}")
        
        # å¯è§†åŒ–
        try:
            plt.figure(figsize=(15, 5))
            
            plt.subplot(1, 3, 1)
            plt.hist(input_lengths, bins=20, alpha=0.7, color='blue')
            plt.xlabel('Input Length (chars)')
            plt.ylabel('Frequency')
            plt.title('Input Length Distribution')
            plt.axvline(np.mean(input_lengths), color='red', 
                       linestyle='--', label=f'Mean: {np.mean(input_lengths):.0f}')
            plt.legend()
            
            plt.subplot(1, 3, 2)
            plt.hist(output_lengths, bins=20, alpha=0.7, color='green')
            plt.xlabel('Output Length (chars)')
            plt.ylabel('Frequency')
            plt.title('Output Length Distribution')
            plt.axvline(np.mean(output_lengths), color='red', 
                       linestyle='--', label=f'Mean: {np.mean(output_lengths):.0f}')
            plt.legend()
            
            plt.subplot(1, 3, 3)
            plt.scatter(input_lengths, output_lengths, alpha=0.6)
            plt.xlabel('Input Length')
            plt.ylabel('Output Length')
            plt.title('Input vs Output Length')
            
            plt.tight_layout()
            plt.savefig('length_distribution.png', dpi=150, bbox_inches='tight')
            print("\nğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: length_distribution.png")
        except Exception as e:
            print(f"\nâš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")
    
    def analyze_medical_terms(self):
        """åˆ†æä¸­åŒ»æœ¯è¯­ä½¿ç”¨æƒ…å†µ"""
        print("\n" + "="*80)
        print("ğŸ¥ ä¸­åŒ»æœ¯è¯­åˆ†æ")
        print("="*80)
        
        # å®šä¹‰ä¸­åŒ»æœ¯è¯­ç±»åˆ«
        term_categories = {
            'è¯å‹': ['é˜´è™š', 'é˜³è™š', 'æ°”è™š', 'è¡€è™š', 'ç—°æ¹¿', 'æ¹¿çƒ­', 
                   'æ°”æ»', 'è¡€ç˜€', 'è‚éƒ', 'è„¾è™š', 'è‚¾è™š', 'å¿ƒç«'],
            'è„è…‘': ['å¿ƒ', 'è‚', 'è„¾', 'è‚º', 'è‚¾', 'èƒƒ', 'è‚ ', 'èƒ†',
                   'è†€èƒ±', 'ä¸‰ç„¦', 'å¿ƒåŒ…'],
            'æ²»åˆ™': ['è¡¥æ°”', 'å…»è¡€', 'æ»‹é˜´', 'æ¸©é˜³', 'æ¸…çƒ­', 'åˆ©æ¹¿',
                   'åŒ–ç—°', 'æ´»è¡€', 'ç†æ°”', 'å®‰ç¥', 'å¥è„¾', 'ç–è‚'],
            'ç—…æœº': ['æ°”è¡€', 'é˜´é˜³', 'å¯’çƒ­', 'è™šå®', 'è¡¨é‡Œ', 'æ´¥æ¶²',
                   'æ°”æœº', 'å‡é™', 'ç»ç»œ', 'å†²ä»»'],
        }
        
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æœ¯è¯­å‡ºç°é¢‘ç‡
        for category, terms in term_categories.items():
            print(f"\nã€{category}ã€‘æœ¯è¯­ä½¿ç”¨é¢‘ç‡:")
            term_counts = Counter()
            
            for item in self.data:
                full_text = item['input'] + item['output']
                for term in terms:
                    count = full_text.count(term)
                    if count > 0:
                        term_counts[term] += count
            
            # æ˜¾ç¤ºTop 5
            for term, count in term_counts.most_common(5):
                percentage = (count / len(self.data)) * 100
                print(f"  {term}: {count}æ¬¡ (å¹³å‡æ¯æ¡{percentage:.1f}%)")
        
        # åˆ†æè¯Šæ–­ç»“æ„å®Œæ•´æ€§
        print(f"\nã€è¯Šæ–­ç»“æ„å®Œæ•´æ€§ã€‘:")
        structure_elements = {
            'ç—‡çŠ¶å½’çº³': ['ç—‡çŠ¶', 'ä¸»ç—‡', 'ä¼´ç—‡'],
            'è¾¨è¯åˆ†æ': ['è¾¨è¯', 'è¯å‹', 'åˆ†æ'],
            'æ²»åˆ™æ²»æ³•': ['æ²»åˆ™', 'æ²»æ³•', 'æ²»ç–—åŸåˆ™'],
            'æ–¹è¯å»ºè®®': ['æ–¹è¯', 'æ–¹å‰‚', 'å¤„æ–¹'],
            'ç”Ÿæ´»è°ƒç†': ['è°ƒç†', 'å»ºè®®', 'æ³¨æ„'],
        }
        
        for element, keywords in structure_elements.items():
            count = sum(1 for item in self.data 
                       if any(kw in item['output'] for kw in keywords))
            percentage = (count / len(self.data)) * 100
            print(f"  åŒ…å«{element}: {count}/{len(self.data)} ({percentage:.0f}%)")
    
    def analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡"""
        print("\n" + "="*80)
        print("âœ… æ•°æ®è´¨é‡åˆ†æ")
        print("="*80)
        
        quality_metrics = {
            'å®Œæ•´æ€§': 0,
            'ä¸“ä¸šæ€§': 0,
            'è¯¦ç»†ç¨‹åº¦': 0,
            'ç»“æ„è§„èŒƒ': 0,
        }
        
        issues = []
        
        for idx, item in enumerate(self.data):
            input_text = item['input']
            output_text = item['output']
            
            # 1. å®Œæ•´æ€§æ£€æŸ¥
            if len(input_text) > 50 and len(output_text) > 200:
                quality_metrics['å®Œæ•´æ€§'] += 1
            else:
                issues.append(f"æ ·æœ¬{idx}: é•¿åº¦ä¸è¶³")
            
            # 2. ä¸“ä¸šæ€§æ£€æŸ¥ï¼ˆåŒ…å«ä¸­åŒ»æœ¯è¯­ï¼‰
            medical_terms = ['è¯å‹', 'è¾¨è¯', 'æ²»åˆ™', 'æ–¹è¯', 'è„è…‘', 
                           'é˜´è™š', 'é˜³è™š', 'æ°”è™š', 'è¡€è™š']
            if any(term in output_text for term in medical_terms):
                quality_metrics['ä¸“ä¸šæ€§'] += 1
            else:
                issues.append(f"æ ·æœ¬{idx}: ç¼ºå°‘ä¸“ä¸šæœ¯è¯­")
            
            # 3. è¯¦ç»†ç¨‹åº¦ï¼ˆè¾“å‡ºæ˜¯å¦è¶³å¤Ÿè¯¦ç»†ï¼‰
            if len(output_text) > 500:
                quality_metrics['è¯¦ç»†ç¨‹åº¦'] += 1
            
            # 4. ç»“æ„è§„èŒƒï¼ˆæ˜¯å¦åŒ…å«åˆ†æç»“æ„ï¼‰
            structure_keywords = ['ç—‡çŠ¶', 'åˆ†æ', 'å»ºè®®', 'æ²»ç–—']
            if sum(kw in output_text for kw in structure_keywords) >= 2:
                quality_metrics['ç»“æ„è§„èŒƒ'] += 1
        
        # æ‰“å°è´¨é‡è¯„åˆ†
        print("\nè´¨é‡è¯„åˆ†ï¼ˆæ»¡åˆ†100ï¼‰:")
        total_samples = len(self.data)
        for metric, count in quality_metrics.items():
            score = (count / total_samples) * 100
            print(f"  {metric}: {score:.1f}åˆ† ({count}/{total_samples})")
        
        avg_score = sum(quality_metrics.values()) / len(quality_metrics) / total_samples * 100
        print(f"\nå¹³å‡è´¨é‡å¾—åˆ†: {avg_score:.1f}åˆ†")
        
        # æ‰“å°é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
        if issues:
            print(f"\nâš ï¸ å‘ç° {len(issues)} ä¸ªæ½œåœ¨é—®é¢˜:")
            for issue in issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  - {issue}")
    
    def analyze_diversity(self):
        """åˆ†ææ•°æ®å¤šæ ·æ€§"""
        print("\n" + "="*80)
        print("ğŸŒˆ æ•°æ®å¤šæ ·æ€§åˆ†æ")
        print("="*80)
        
        # 1. ç—‡çŠ¶å¤šæ ·æ€§
        print("\nã€ç—‡çŠ¶ç±»å‹å¤šæ ·æ€§ã€‘:")
        symptom_categories = {
            'ç–¼ç—›': ['ç–¼', 'ç—›', 'é…¸'],
            'ç–²åŠ³': ['ç´¯', 'ä¹åŠ›', 'ç–²åŠ³'],
            'ç¡çœ ': ['å¤±çœ ', 'å¤šæ¢¦', 'ç¡çœ '],
            'æ¶ˆåŒ–': ['é£Ÿæ¬²', 'è…¹', 'ä¾¿'],
            'å¾ªç¯': ['å¿ƒæ‚¸', 'èƒ¸é—·', 'å‡ºæ±—'],
            'æ¸©åº¦': ['å†·', 'çƒ­', 'å‘çƒ§'],
        }
        
        for category, keywords in symptom_categories.items():
            count = sum(1 for item in self.data 
                       if any(kw in item['input'] for kw in keywords))
            percentage = (count / len(self.data)) * 100
            print(f"  {category}ç›¸å…³: {count}æ¡ ({percentage:.0f}%)")
        
        # 2. è¯å‹å¤šæ ·æ€§
        print("\nã€è¯å‹åˆ†å¸ƒã€‘:")
        è¯å‹åˆ—è¡¨ = ['é˜´è™š', 'é˜³è™š', 'æ°”è™š', 'è¡€è™š', 'æ¹¿çƒ­', 'ç—°æ¹¿', 
                 'æ°”æ»', 'è¡€ç˜€', 'è‚éƒ', 'è„¾è™š']
        è¯å‹åˆ†å¸ƒ = Counter()
        
        for item in self.data:
            for è¯å‹ in è¯å‹åˆ—è¡¨:
                if è¯å‹ in item['output']:
                    è¯å‹åˆ†å¸ƒ[è¯å‹] += 1
        
        for è¯å‹, count in è¯å‹åˆ†å¸ƒ.most_common():
            percentage = (count / len(self.data)) * 100
            print(f"  {è¯å‹}: {count}æ¡ ({percentage:.0f}%)")
        
        # 3. è¯æ±‡å¤šæ ·æ€§ï¼ˆä½¿ç”¨TTR - Type-Token Ratioï¼‰
        print("\nã€è¯æ±‡å¤šæ ·æ€§ã€‘:")
        all_words = []
        for item in self.data:
            text = item['input'] + item['output']
            words = list(jieba.cut(text))
            all_words.extend(words)
        
        unique_words = set(all_words)
        ttr = len(unique_words) / len(all_words)
        
        print(f"  æ€»è¯æ•°: {len(all_words):,}")
        print(f"  ç‹¬ç‰¹è¯æ•°: {len(unique_words):,}")
        print(f"  è¯æ±‡å¤šæ ·æ€§(TTR): {ttr:.3f}")
        print(f"  ï¼ˆTTRè¶Šé«˜è¡¨ç¤ºè¯æ±‡è¶Šä¸°å¯Œï¼Œä¸€èˆ¬>0.6ä¸ºå¥½ï¼‰")
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š ä¸­åŒ»é—®è¯Šæ•°æ®é›† - å®Œæ•´åˆ†ææŠ¥å‘Š")
        print("="*80)
        
        print(f"\næ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
        print(f"  æ•°æ®æ¥æº: {self.data_path}")
        print(f"  æ ·æœ¬æ€»æ•°: {len(self.data)}")
        print(f"  æ•°æ®ç±»å‹: ä¸­åŒ»é—®è¯Šå¯¹è¯")
        
        # ä¾æ¬¡æ‰§è¡Œå„é¡¹åˆ†æ
        self.analyze_length_distribution()
        self.analyze_medical_terms()
        self.analyze_data_quality()
        self.analyze_diversity()
        
        print("\n" + "="*80)
        print("âœ… åˆ†æå®Œæˆï¼")
        print("="*80)
        
        print("\nğŸ’¡ æ•°æ®é›†è¯„ä»·:")
        print("  âœ… ä¼˜ç‚¹:")
        print("    - æ•°æ®çœŸå®å®Œæ•´ï¼ŒåŒ…å«è¯¦ç»†çš„é—®è¯Šæµç¨‹")
        print("    - ä¸­åŒ»æœ¯è¯­ä½¿ç”¨ä¸“ä¸šï¼Œç¬¦åˆä¸­åŒ»è¯Šæ–­è§„èŒƒ")
        print("    - å›å¤ç»“æ„åŒ–ï¼ŒåŒ…å«ç—‡çŠ¶åˆ†æã€è¾¨è¯ã€æ²»ç–—ç­‰å®Œæ•´å†…å®¹")
        print("    - æ•°æ®é•¿åº¦é€‚ä¸­ï¼Œé€‚åˆæ¨¡å‹å­¦ä¹ ")
        
        print("\n  âš ï¸ å¯ä»¥æ”¹è¿›çš„åœ°æ–¹:")
        print("    - å¯ä»¥å¢åŠ æ›´å¤šæ ·æœ¬æ•°é‡ï¼ˆå»ºè®®>1000æ¡ï¼‰")
        print("    - å¯ä»¥æ‰©å±•æ›´å¤šè¯å‹å’Œç—…ç§")
        print("    - å¯ä»¥åŠ å…¥æ›´å¤šå¤šè½®å¯¹è¯åœºæ™¯")


def main():
    """ä¸»å‡½æ•°"""
    analyzer = TCMDatasetAnalyzer('tcm_raw_data.json')
    analyzer.generate_report()


if __name__ == "__main__":
    main()

